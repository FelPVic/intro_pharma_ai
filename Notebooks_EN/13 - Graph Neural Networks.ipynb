{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5efb70",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "\n",
    "---\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "* how molecules can be represented as graphs. \n",
    "* how basic Graph Neural Networks work.\n",
    "* how to write a Graph Neural Network as a Pytorch class.\n",
    "---\n",
    "\n",
    "\n",
    "Graph Neural Networks are still a relatively new class of algorithms. Intuitively, molecules can be represented very easily as a (mathematical) graph. The bonds of a molecule correspond to the edges of the graph and the atoms to the nodes. <br> For computers, however, graphs are not as easy to read as, for example, a Smiles `string`. For graph neural networks we need at least two matrices to adequately represent moleculs. One is the adjacency matrix, which represents the connections between the atoms (i.e. the bonds). The other matrix is the feature matrix. This is where information about individual atoms can be stored.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Jorge_Galvez2/publication/236018587/figure/fig1/AS:299800013623305@1448489301609/The-chemical-graph-and-adjacency-matrix-of-the-isopentane.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Galvez et. al. 2010</center></h8><br><br>\n",
    "\n",
    "For today's example we use again the data from the Tox21 Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "from os.path import exists\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install rdkit==2022.3.4\n",
    "    if exists(\"utils.py\") == False:\n",
    "        !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/onehotencoder.py\n",
    "    %run onehotencoder.py\n",
    "else:\n",
    "    %run ../utils/onehotencoder.py\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "np.set_printoptions(linewidth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten\n",
    "data_tox = pd.read_csv(\"https://raw.githubusercontent.com/filipsPL/tox21_dataset/master/compounds/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] # all columns except the first one (index 0) are selected\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321a720",
   "metadata": {},
   "source": [
    "## Adjacency Matrix and One-Hot Encoded Feature Matrix\n",
    "\n",
    "Unfortunately, we can't really use moleucles represented as Smiles this time. We need convert molecules into a graph representation. RDKit provides some functionalities that make it easier to work with graphs. For example, there is a function that creates adjacency matrices for molecules. We have already imported this one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [Chem.MolFromSmiles(x) for x in data_tox['smiles']]\n",
    "A = [GetAdjacencyMatrix(x) for x in mols]\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0e03f",
   "metadata": {},
   "source": [
    "As features for the atoms we use only the atomtype. We will also one-hot encode these.\n",
    "\n",
    "For the one-hot encoding of the atoms we use the already written function `onehotencode()`.\n",
    "Similar to the encoding of Smiles tokens in RNNs, only the atomtype of each atom is recorded here. These atoms are then represented as one-hot encoded vectors.\n",
    "Per molecule, these vectors are combined to form a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fb9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = onehotencode(_____)\n",
    "feat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569c2a7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "feat = onehotencode(mols)\n",
    "feat[1]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926e98c",
   "metadata": {},
   "source": [
    "Oben können Sie sehen, wie eine Featurematrix für ein Molekül aussieht.\n",
    "Wenn wir uns die `.shape` ansehen, können wir sehen, dass dieses Molekül aus `29` Atomen besteht. Die Anzahl der Spalten lässt uns wissen, dass es insgesamt 25 Atomtypen im Datensetz gibt. Die Anzahl der Spalten (Anzahl der Features) muss für alle Moleküle gleich sein, sonst können wir die Moleküle nicht durch das Netzwerk führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4961d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd903",
   "metadata": {},
   "source": [
    "You may have noticed that there are still zeros on the diagonal of the adjacency matrix. In a graph convolution, however, not only the features of the neighboring atoms but also those of the central atom are to be included in the aggregation. For this, ones are needed on the diagonal of the adjacency matrix. \n",
    "With the function `np.fill_diagonal(matrix, value)` you can change the values of the diagonals of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for matrix in A:\n",
    "    np.fill_diagonal(matrix, 1)\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ff2ee",
   "metadata": {},
   "source": [
    "## Graph Convolution\n",
    "\n",
    "Now we want to pass the information of each node to the neighboring nodes along the edges. This is called a graph convolution. One way to this, is using the following operation for the forward pass: $$\\hat{X} = \\hat{D}^{-1}\\hat{A}XW$$.\n",
    "\n",
    "Here $\\hat{A}$ is our adjacency matrix, with ones on the diagonal. $X$ is the feature matrix and $W$ are the weights that Pytorch initializes. What we are still missing is $D$, the $D$egreen matrix. This matrix contains the number of bonds each atom has in the molecule. These values are placed on the diagonal of the degreematrix.\n",
    "The number of bonds of each molecule can be easily calculated from the adjacency matrix. To do this, one must sum over the rows or columns of the matrix $\\hat{A}$. This sum gives us the degree of an atom. To be exact, it is the degree plus one, since you have already filled the diagonal of the adjacency matrix with ones.\n",
    "To create the degree matrix, you must calculate the sums of the columns in each adjacency matrix and place them on the diagonal of a new matrix.\n",
    "\n",
    "\n",
    "This process is relatively easy to do with `numpy`. `np.sum(A[i], axis=0)` calculates the sum per column and `np.diag()` creates a matrix from a 1D array with the values of the 1D array on the diagonal. You can use the two functions in combination to create the degreematrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bef88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D =[]\n",
    "for matrix in A:\n",
    "    D.append(np.diag(np.sum(matrix, axis=1)))\n",
    "print(D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2560b",
   "metadata": {},
   "source": [
    "\n",
    "But we do not need $\\hat{D}$. We need the inverse of this matrix. Without $\\hat{D}^{-1}$, $\\hat{A}X$ would sum the features across all connected nodes. This would result in atoms with more neighbors having larger feature values. By including $\\hat{D}^{-1}$, the aggregated features are averaged by the number of neighboring atoms. `np.linalg.inv()` is needed to get the inverse of the matrix `D`.\n",
    "\n",
    "Before we start building a network, we need to perform one more step. To save computational effort, we can precompute $\\hat{D}^{-1}\\hat{A}$ before training the netowrk. This can save time since this step will not be repeatedly computed during training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159329",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = []\n",
    "for i in range(len(D)):\n",
    "    DA.append(np.matmul(np.linalg.inv(D[i]),A[i]))\n",
    "DA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fe5a8",
   "metadata": {},
   "source": [
    "We now have the list of adjacency matrices `DA`, contains the information about the structure of the molecule. The list `feat` contains the features, i.e. the information about the individual atoms. And finally, we create a list `labels`. This list contains the information that we want to predict (toxic vs. non-toxic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = [torch.tensor(x,dtype=torch.float32) for x in DA] \n",
    "feat = [torch.tensor(x,dtype=torch.float32) for x in feat] \n",
    "labels = [torch.tensor([x], dtype=torch.float32) for x in data_tox['activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd3802",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer\n",
    "\n",
    "We want to take advantage of PyTorch for graph convolution. Like last week, we will use classes to do this.\n",
    "We used a class to create a network  made out of different layers (e.g. `nn.Linear()` or `nn.GRU`). \n",
    "These layers are already given in PyTorch, but we can also create our own layers. For this we need to write a new PyTorch class.\n",
    "\n",
    "Our goal is to program something like `nn.Linear()` so that we can add graph convolution to our repertoire of layers (Linear, RNN, GRU, Dropout, ...).\n",
    "\n",
    "Again we can use the class `nn.Module` as a basis for our Graph Convolution. \n",
    "We start with the minimal framework:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class GraphConvolution(nn.Module):\n",
    "    pass\n",
    "```\n",
    "\n",
    "\n",
    "In order to properly access the functions of the parent `nn.Module` class, we need to initialize it with `super().__init__()`. Of course, our convolutional layer needs to know how big the input is, and how big the output should be. \n",
    "\n",
    "\n",
    "```python\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_featuers = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "```\n",
    "\n",
    "Now we can write the `forward()` function. Here the actual convolution happens, where the nodes with the features `x` are aggregated over their neighboring nodes using a matrix multiplication with the adjacency matrix `adj`. By including the learnable `weights` we can optimize the convolution during training.\n",
    "\n",
    "```python\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "```\n",
    "With this, our `GraphConvolution` class would already work. But since we do not define a network now, but a new layer, we must additionally specify how the weights and biases are to be initialized.\n",
    "We initialize the weights with the function `reset_parameters()`. The new function `__repr__()` provides a  graphically overview what our layer looks like after it has been initialized. \n",
    "\n",
    "\n",
    "Our final class will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e85d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(0, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'in_features=' + str(self.in_features) + ', ' \\\n",
    "               + 'out_features=' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d289f",
   "metadata": {},
   "source": [
    "We can also check if our Graph Convolution Layer is already working.\n",
    "First, we save a signle feature matrix and an adjacency matrix as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_example = feat[1]\n",
    "adj_example = DA[1]\n",
    "print('Features:', feat_example.shape)\n",
    "print('DA:', adj_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b55b",
   "metadata": {},
   "source": [
    "We now initialize a `GraphConvolution`. Note that the input size, of the first layer is equal to the size of the feature vector (number of columns) (`25`). In that regard it behaves like a `nn.Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GraphConvolution(25, 100)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab305ee3",
   "metadata": {},
   "source": [
    "We can now take the example through convolution. You will see that the dimension of the features will have increased to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(feat_example, adj_example)\n",
    "print('\\nOutput:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a666",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "\n",
    "To create a network now, we can use the class `nn.Module` again. \n",
    "\n",
    "Last week we put together our own autoencoder. This week we use this type of class to connect multiple graph convolutions together. However, it is important to note that, as with regular CNNs, we cannot use only convolution layers. In the CNN itself, we need to \"flatten\" the tensor at the end. This gives us a vector that is then passed through a linear layer. The same is true for graph convolutions. However, to pass the output of these graph convolutions into a linear layer, we cannot simply use PyTorch's `flatten`, but we will take the average across all columns.\n",
    "\n",
    "This is done with the function `aggregate()`.\n",
    "\n",
    "You can see in the network we use two graph convolution and then a linear layer.\n",
    "In the forward pass, the input `(x, adj)` is first passed through the inital graph convolution, then a ReLU function is applied. The same is then repeated for the second convolution. \n",
    "Now we apply the function `aggregate`. This calculates the mean value of each feature. Thus the output of this layer has the size `[1,100]` in the example from above.\n",
    "Finally, we use the linear layer to make the actual prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):#in_features, out_features, size_labels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): # we use mean aggregation, max or min could also be used as hyperparameter\n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8283147",
   "metadata": {},
   "source": [
    "Now we will quickly divide the dataset into a training set and a test set. To do this, we simply use the first 1800 molecules as the training set and the rest as the test set. It is important that we do not use minibtaches in this example. The reason for this is that the feature matrices of each molecule are of different sizes (have different numbers of rows). This means that we cannot simply store them as a 3D tensor, as is the case with CNNs, for example. \n",
    "\n",
    "There are ways to solve this problem, but they are not relevant for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = feat[:1800]\n",
    "train_DA = DA[:1800]\n",
    "train_labels = labels[:1800]\n",
    "\n",
    "\n",
    "test_feat = feat[1800:]\n",
    "test_DA = DA[1800:]\n",
    "test_labels = labels[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b44562",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNN()\n",
    "loss_function= nn.BCEWithLogitsLoss()\n",
    "optimizer=optim.Adam(gnn.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list_train = []\n",
    "    acc_list_train= []\n",
    "    gnn.train()\n",
    "    for k in range(len(train_feat)):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output=gnn(train_feat[k], train_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_function(output,train_labels[k])\n",
    "        loss.backward()\n",
    "        loss_list_train.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_list_train.append(np.sum((torch.round(torch.sigmoid(output)) == train_labels[k]).detach().numpy()))\n",
    "    loss_list_test = []\n",
    "    acc_list_test= []\n",
    "    gnn.eval()\n",
    "    for k in range(len(test_feat)):\n",
    "    \n",
    "        output=gnn(test_feat[k], test_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_function(output,test_labels[k])\n",
    "        loss_list_test.append(loss.item())\n",
    " \n",
    "\n",
    "        acc_list_test.append(np.sum((torch.round(torch.sigmoid(output)) == test_labels[k]).detach().numpy()))\n",
    "            \n",
    "        \n",
    "    print(i,\"Train Loss: %.2f Train Accuracy: %.2f Test Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list_train), np.mean(acc_list_train),np.mean(loss_list_test), np.mean(acc_list_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d732ba",
   "metadata": {},
   "source": [
    "As you can see, the training takes a long time and is not very successful. The model presented here is a very, very simple model. In fact, more complex graph convolutions are usually used. Additionally, a variety of features, not only the atomtype,are used as input in the feature matrix. \n",
    "\n",
    "For more complex Graph Neural Netwroks libraries such as [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) or [Deep Graph Library](https://www.dgl.ai/) exist. Both are an extension to PyTorch. They contain important functionalities that are relevant for dealing with graphs. In addition, the libraries contain the most important graph layers. This way you do not have to program the layers yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe796496",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Here you can see our Graph Neural Network.\n",
    "The problem is that this network offers no flexibility.\n",
    "The weight matrices of the network will always have the same dimensions.\n",
    "Dropout is also missing. This time we don't need a batchnorm, tho, because we don't use minibatches.\n",
    "\n",
    "\n",
    "```python\n",
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "```\n",
    " \n",
    "Can you rewrite the network so that we can flexibly adjust the input as well as the sizes of the graph convolutions?\n",
    "You can test if your network still works with the `example_DA` and `example_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_DA = test_DA[0]\n",
    "example_feat = test_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23046cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
