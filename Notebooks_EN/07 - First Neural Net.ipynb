{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Neuronal Network\n",
    "---\n",
    "### In this lesson you'll learn:\n",
    "\n",
    "- how one-hot encoding works.\n",
    "- how to program a simple neural net in Python.\n",
    "---\n",
    "\n",
    "Before we train the neural network, the first thing we need to do is load the relevant libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) # np.exp() = e^()\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "We want the labels that we are training the netowrk on, to be a vector of zeros and ones rather than a simple single number. An example is shown below:<br>\n",
    "<center>\n",
    "0  =  [1 0 0 0 0 0 0 0 0 0]<br>\n",
    "1  =  [0 1 0 0 0 0 0 0 0 0]<br>\n",
    "2  =  [0 0 1 0 0 0 0 0 0 0]<br>\n",
    "3  =  [0 0 0 1 0 0 0 0 0 0]<br>\n",
    "4  =  [0 0 0 0 1 0 0 0 0 0]<br>\n",
    "5  =  [0 0 0 0 0 1 0 0 0 0]<br>\n",
    "etc. <br>\n",
    "9  =  [0 0 0 0 0 0 0 0 0 1]<br>\n",
    "</center>\n",
    "\n",
    "The `one-hot` function does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    one_hot_matrix = np.zeros(\n",
    "        [\n",
    "            len(labels),\n",
    "            len(set(labels))\n",
    "        ]\n",
    "    )\n",
    "    for i,x in enumerate(labels):\n",
    "        one_hot_matrix[i,x] = 1\n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales the original images, which can have values between 0 and 255, to images with values between 0 and 1.\n",
    "# This allows neural networks to be trained more easily.\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Today we will work with the MNIST Dataset \\[1\\]. It is likely the most famous dataset with regards to deep learning. \n",
    "It contains image of handwritten digits between zero and nine. The aim is to train a model to correctly recognize these digits.\n",
    "\n",
    "We can easily read the required data with `numpy`. The training data `mnist_train.csv` contains images and their labels. The images have already been transformed from a matrix to a vector.<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" style=\"width: 800px;\">\n",
    "</center>\n",
    "\n",
    "\n",
    "The data set can also be viewed externally. You can also view the complete dataset. You only need to copy the link `'https://uni-muenster.sciebo.de/s/xSU1IKM6ui4WKAV/download'` into your browser and it will download the file.  The first column of each data set contains the labels. In each row there is an image, furthermore each column (except the first one) has the values of a specific pixel assigned to it.\n",
    "\n",
    "With `np.genfromtxt()` arrays can be created from `.txt` files in Python. \n",
    "\n",
    "\n",
    "In this example, the labels or targets are in the first column, not the last.\n",
    "\n",
    "---\n",
    "\\[1\\] Deng, L. (2012). The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), 141â€“142."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('https://uni-muenster.sciebo.de/s/HvLen05c6fIzmyX/download', delimiter=',', skip_header =False) \n",
    "# genfromtxt reads .txt files, setting delimiter =\",\" allows to read .csv (comma seperated values) files.\n",
    "test_data=np.genfromtxt('https://uni-muenster.sciebo.de/s/xdpCHFmmCAfeuxh/download', delimiter=',', skip_header =False) \n",
    "# Here we load the test data.\n",
    "\n",
    "# After we have imported the data, we divide the data into images and labels.\n",
    "# We also convert labels from float to integer with .astype(int).\n",
    "train_labels=train_data[:,0].astype(int) \n",
    "train_images = train_data[:,1:]\n",
    "\n",
    "test_labels=test_data[:,0].astype(int)\n",
    "test_images = test_data[:,1:]\n",
    "\n",
    "del train_data, test_data # for more memory we delete original data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now read in the training dataset. With `train_images.shape` you can see that the variable `train_images` is a 60000 x 784 matrix, so it has 60000 rows and 784 columns. Each row is an image and each column is a pixel. The original images were 28 x 28 pixels. In the next step we use the *function* `one_hot` to encode the labels correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets=one_hot(train_labels)\n",
    "test_targets = one_hot(test_labels)\n",
    "\n",
    "train_targets[:5,:] # the labels of the first five images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to use the `min_max` scaler to scale the pixel values between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = min_max(train_images)\n",
    "test_images = min_max(test_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell you can see an example image. The function `.reshape([28,28])` will change the image back to its original form. This way you can actually see the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0].reshape([28, 28]), cmap=\"gray\")\n",
    "print(\"Correct label: %s\" % train_labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "\n",
    "**Initializing the weights**\n",
    "\n",
    "First, the weight and bias matrices/vectors must be created with the correct dimensions. For this purpose we'll write a function. In it we have to define the size of the respective layers (`input_size`, `hidden_size`, `output_size`). With these you can create the weight matrices. While the `b` vectors can be filled with zeros, the weights matrices must be initialized with random small numbers.\n",
    "\n",
    "\n",
    "The function `np.random.randn(number_of_rows, number_of_columns)` generates random values between minus one and one and adds them to the matrix. Additionally the weights are multiplied with the following term `*np.sqrt(2/layer_size)`. This reduces the values even more and should guarantee a better training of the network. \n",
    "\n",
    "In the following code you should enter the correct matrix sizes (number of rows/columns). The input matrix `X` has 60000 rows and 784 columns and for a matrix multiplication the number of columns of the first matrix must match the number of rows of the second matrix.\n",
    "Also remember that we will use the transpose of the weight matrix.\n",
    "\n",
    "The bias `b` is initialized with zeros. `np.zeros(500)` would fill a vector of length 500 with zeros.\n",
    "Also, we do not need to mention the actual numbers, but only the names of the input variables, so not 784, but `input_size`.\n",
    "The actual values can then be defined when actually using the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function initializing the weights\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # two empty lists with a lenght of two\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # here the weights W are initialized with small, random numbers\n",
    "    \n",
    "    W[0] = np.random.randn(        ,         ) * np.sqrt(2 / input_size)  # WRITE THE CORRECT SIZES HERE\n",
    "    W[1] = np.random.randn(        ,         ) * np.sqrt(2 / hidden_size) # WRITE THE CORRECT SIZES HERE\n",
    "\n",
    "    # the bias can be zero\n",
    "    b[0] = np.zeros(       ) # WRITE THE CORRECT SIZES HERE\n",
    "    b[1] = np.zeros(       ) # WRITE THE CORRECT SIZES HERE\n",
    "\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "def init_weights(input_size, hidden_size, output_size):\n",
    "    # two empty lists with a lenght of two\n",
    "    b = [0] * 2\n",
    "    W = [0] * 2\n",
    "\n",
    "    # here the weights W are initialized with small, random numbers\n",
    "    \n",
    "    W[0] = np.random.randn(hidden_size,input_size) * np.sqrt(2 / input_size)  \n",
    "    W[1] = np.random.randn(output_size,hidden_size) * np.sqrt(2 / hidden_size) \n",
    "\n",
    "    # the bias can be zero\n",
    "    b[0] = np.zeros(hidden_size) \n",
    "    b[1] = np.zeros(output_size) \n",
    "\n",
    "\n",
    "    return W, b\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights can now be initialized: \n",
    "\n",
    "The input size is predefined, since each image has 784 pixels. The output size is also predefined, since the net must distinguish between 10 different digits.\n",
    "The only value you can set yourself is the size of the hidden layers. There is no simple rule for the size, but if it is too small, it can reduce the accuracy of the neural network. If it is too large, the network will train too slowly and may become unstable. This can also affect the accuracy. Try values like 100 or 200 in the beginning. \n",
    "\n",
    "Parameters like the size of the hidden layers or the learning rate, which you can freely choose, are called **hyper parameters**. Unlike the actual weights of the net, these are not optimized by the backpropagation. This has to be done \"by hand\". This process is called hyperparameter optimization.\n",
    "\n",
    "With this function you can now initialize the weights. The function outputs two lists `W` and `b`. The two lists contain two matrices/vectors each, one for the first and one for the second transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = init_weights(input_size=784, hidden_size=200,output_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialized the weights correctly, `W[0].shape` should be `(200,784)`.\n",
    "\n",
    "---\n",
    "\n",
    "You may ask: Why don't we just initialize our weights with ` W[0] = np.random.randn(input_size,hidden_size)`. Then we wouldn't need to use `.transpose()` later, since the matrices already have the correct format.\n",
    "\n",
    "Indeed, this is possible, but this way of initialization has become the standard. To avoid future confusion, we follow this standard as well. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward pass** \n",
    "\n",
    "After the weights have been initialized, you can perform the forward pass of the network. The images are sent through the network, which as the last step classifies these images.\n",
    "The function `forward_pass()` does just that. Its input is:\n",
    "\n",
    "* `W`: list of weight matrices\n",
    "* `b`: list of bias vectors\n",
    "* `X`: input matrix of images \n",
    "\n",
    "\n",
    "\n",
    "First $Z_1 = xW_1^T+b_1$ is calculated. <br>\n",
    "Then $A_1= sigmoid(Z_1)$.\n",
    "\n",
    "<br>\n",
    "This is how the activations for the hidden layers are calculated. For the final classification, the values must be transformed a second time:<br>\n",
    "$\\hat{Y} = softmax(A_1W_2^T+b_2)$\n",
    "\n",
    "The function outputs three variables at the end:\n",
    "* `Z_1`: the values of the first linear transformation\n",
    "* `A_1`: the values after the first activation function\n",
    "* `Y_hat`: the values of the output layer \n",
    "\n",
    "\n",
    "*Remember that the weights of the first layers are not stored in* `W[1]` *but in* `W[0]`*, because the indexing starts at zero and not at one.*\n",
    "\n",
    "*Remember also that for matrix multiplication we use* `np.matmul` *and no longer* `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = # CODE TO CALCULATE Z_1 \n",
    "    A_1 = # CODE TO CALCULATE A_1 \n",
    "    Z_2 = # CODE TO CALCULATE Z_2 \n",
    "    Y_hat = #CODE TO CALCULATE Y_HAT \n",
    "    return Z_1, A_1, Y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "def forward_pass(W, b, X):\n",
    "    \n",
    "    Z_1 = np.matmul(X,W[0].transpose())+b[0] \n",
    "    A_1 = sigmoid(Z_1) \n",
    "    Z_2 = np.matmul(A_1,W[1].transpose())+b[1] \n",
    "    Y_hat = softmax(Z_2) \n",
    "    return Z_1, A_1, Y_hat\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "After the forward pass, the loss is calculated. It measures how well or poorly the model was able to recognize the numbers in the images.\n",
    "\n",
    "For this only the values of the output layer (`y_hat`) and the true values (`y`) are used. The true values were created before with the function `one_hot` and are stored in the variable `train_targets`. \n",
    "\n",
    "You have already learned about the Binary Crossentropy Loss, which is used for binary classification. This loss function is just a special version of the Crossentropy Loss, which can be used for the classification of more than two classes. Since we are now trying to distinguish ten different numbers, we will continue with the more general Crossentropy Loss. \n",
    "\n",
    "The exact loss function is given here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_hat, y):\n",
    "    return -np.sum(np.log(y_hat) * y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the first three functions together to perform your first classification.\n",
    "Remember that the `train_images` are the input to the network. We want the hidden layer to have 300 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) # A seed is set so that the results of the random initialization are the same for all participants.\n",
    "W, b = # CODE TO INITIALIZE THE WEIGHTS\n",
    "\n",
    "# Use the train_images as X (input)\n",
    "Z_1, A_1, Y_hat = # CODE FOR THE FORWARD PASS\n",
    "\n",
    "# Here you calculate the loss\n",
    "calc_loss(           ,           )/Y_hat.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784,300,10)\n",
    "\n",
    "# Use the train_images as X (input)\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "    \n",
    "# Here you calculate the loss\n",
    "calc_loss(Y_hat,train_targets)/Y_hat.shape[0]\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additionally calculate the accuracy to get better overview of how well our model works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_labels,predicted):\n",
    "    pred_labels = np.argmax(predicted, axis=1) # argmax returns the index of the maximum value\n",
    "    correct_predicted = np.sum(true_labels == pred_labels)\n",
    "    return correct_predicted /true_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(train_labels,Y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the network has an accuracy of 10.3%. An accuracy of 10% can be expected if the network decides randomly.\n",
    "To achieve a better accuracy above 10%, you have to train the network or change the weights. Use the backpropagation for this purpose.\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "The errors in the classification are now fed back through the network, and based on the gradients, the weights are adjusted.\n",
    "\n",
    "* $dZ_2 = \\hat{y} - y$ \n",
    "* $dW_2 = \\frac{1}{n} \\cdot dZ_2^Ta_1$\n",
    "* $db_2 = \\frac{1}{n} \\cdot \\sum_{i=1}^n dZ_2$\n",
    "* $dZ_1 = dZ_2W_2 \\cdot a_1 \\cdot (1-a_1)$\n",
    "* $dW_1 = \\frac{1}{n} \\cdot dZ_1^TX$\n",
    "* $db_1 = \\frac{1}{n} \\cdot\\sum_{i=1}^n dZ_1$\n",
    "\n",
    "How the gradients are mathematically obtained is beyond the scope of these exercises. For the concrete application in later exercises, there are libraries that can calculate the gradients automatically. For the moment, however, we calculate the gradients ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n is the number of images\n",
    "    \n",
    "    # Gradients for the weights of the second layer\n",
    "    dZ_2 =  # CODE TO CALCULATE dZ_2\n",
    "    dW_2 =  # CODE TO CALCULATE dW_2\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients for the weights of the first layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = # CODE TO CALCULATE dW_1\n",
    "    db_1 = # CODE TO CALCULATE db_1\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] # Here again two lists are returned, in each of the lists are the gradients for W_1,W_2 and b_1 and b_2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "def back_prop(X, Z_1, A_1, Y_hat, y):\n",
    "    n = X.shape[0] # n is the number of images\n",
    "\n",
    "    # Gradients for the weights of the second layer\n",
    "    dZ_2 = Y_hat - y\n",
    "    dW_2 = np.matmul(dZ_2.transpose(), A_1) / n\n",
    "    db_2 = np.sum(dZ_2, axis=0) / n\n",
    "    \n",
    "    # Gradients for the weights of the first layer\n",
    "    dZ_1 = np.multiply(np.matmul(dZ_2, W[1]), np.multiply(A_1, 1 - A_1))\n",
    "    dW_1 = np.matmul(dZ_1.transpose(), X) / n\n",
    "    db_1 = np.sum(dZ_1, axis=0) / n\n",
    "\n",
    "    return [dW_1, dW_2], [db_1, db_2] \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights\n",
    "\n",
    "In the last step, the weights are adjusted. For this purpose, the weights are shifted a little against the gradient. \n",
    "How far exactly depends on the learning rate `lr`. The larger the learning rate, the larger the steps.\n",
    "If the learning rate is too small, the training may take too long or the mesh may get stuck in a local minimum. If the learning rate is too large, the loss of the network jumps too much, so optimal performance cannot be guaranteed.\n",
    "The learning rate, like the size of the hidden layers, is a hyperparameter.\n",
    "<center>\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_learning_rate.png\" style=\"width: 800px;\">\n",
    "<h8><center>Source: Sebastian Raschka, https://sebastianraschka.com</center></h8>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = # CODE FOR W[0]\n",
    "    W[1] = # CODE FOR W[1]\n",
    "    b[0] = # CODE FOR b[0]\n",
    "    b[1] = # CODE FOR b[1]\n",
    "\n",
    "    return W, b # the function returns the new weights and biases (2 listn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "def update(W, b, grad_W, grad_b, lr=0.0001):\n",
    "    W[0] = W[0] - lr * grad_W[0]\n",
    "    W[1] = W[1] - lr * grad_W[1]\n",
    "    b[0] = b[0] - lr * grad_b[0]\n",
    "    b[1] = b[1] - lr * grad_b[1]\n",
    "\n",
    "    return W, b # the function returns the new weights and biases (2 listn)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now you can put everything together. First you initialize the weights, then the input is passed through the network and the loss is calculated.  Then the loss is passed back through the net and the gradients are calculated. Using the gradients, the weights are updated. For the learning rate use the value 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.INITIALIZE WEIGHTS\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.CALCULATE LOSS\n",
    "print(\"Loss after first pass:\",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nAccuracy after first pass:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = # CODE FOR BACKPROPAGATION\n",
    "\n",
    "# 5. UPDATE WEIGHTS\n",
    "W, b = # NEW WEIGHTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "# 1.INITIALIZE WEIGHTS\n",
    "np.random.seed(1234) \n",
    "W, b = init_weights(input_size=784, hidden_size=300,output_size= 10) \n",
    "\n",
    "# 2.FORWARD PROPAGATION\n",
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# 3.CALCULATE LOSS\n",
    "print(\"Loss after first pass:\",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nAccuracy after first pass:\", accuracy(train_labels, Y_hat) )\n",
    "\n",
    "# 4. BACKPROPAGATION\n",
    "grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "\n",
    "# 5. UPDATE WEIGHTS\n",
    "W, b = update(W, b, grad_W, grad_b, lr = 0.1)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss and Accuracy are still unchanged. Only when you send the input through the network again, you can see the effect of the updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_1, A_1, Y_hat = forward_pass(W,b,train_images)\n",
    "\n",
    "# now calculate the loss\n",
    "print(\"Loss after the second pass:\",calc_loss(Y_hat,train_targets)/Y_hat.shape[0], \"\\nAccuracy after the second pass:\", accuracy(train_labels, Y_hat) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the loss decreases and the accuracy improves. However, short-term deterioration may also occur during training, but this is not a big deal.  The effect of the training often becomes visible only after several epochs. You can simply repeat the training step.\n",
    "To do this more efficiently, simply write a `for-loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.INITALIZE WEIGHTS \n",
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "\n",
    "EPOCHS= 50 # how often the data is passed through the network\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    # 2. FORWARD PROPAGATON\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    # 3. CALCULATE LOSS\n",
    "    loss = calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # 4. BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    \n",
    "    # 5. UPDATE WEIGHTS\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see an improvement and achieve an accuracy of 73%. However, the training takes a long time. With a higher learning rate, the training should be faster. Try a learning rate of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 50 # how often the data is passed through the network\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    \n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    \n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    \n",
    "    # UPDATE WEIGHTS\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a learning rate of 0.3, you achieve an accuracy of 78% after 50 epochs. You can improve the net even further by training for even longer. \n",
    "\n",
    "What would you need to change in the code to train for 25 more epochs **without having to train the net again from the beginning**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # how often the data is passed through the network\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # UPDATE WEIGHTS\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution:</strong></summary>\n",
    "\n",
    "```python\n",
    "np.random.seed(1234)\n",
    "# W, b = init_weights(784, 300, 10) # Simply do not initialize the weights again.\n",
    "loss= []\n",
    "EPOCHS= 25 # how often the data is passed through the network\n",
    "for i in range(EPOCHS):\n",
    "    Z_1, A_1, Y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(Y_hat, train_targets) / Y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, Y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # BACK PROPAGATION\n",
    "    grad_W, grad_b = back_prop(train_images, Z_1, A_1, Y_hat, train_targets)\n",
    "    # UPDATE WEIGHTS\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You simply must not reinitialize the weights. Otherwise, everything that the net has already learned will be lost.\n",
    "\n",
    "With further training, you have been able to improve the mesh by 6% Accuracy. Basically, you still have the option to change something about the size of the hidden layers. \n",
    "\n",
    "Before we do that, you can first see how well our model works on images it hasn't seen yet. We are talking about the test dataset here. For this purpose, the test set is fed into the trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_y_hat = forward_pass(W, b, test_images) # by using underscores, z_1 and a_1 aren't returned, we don't need them\n",
    "accuracy(test_labels, test_y_hat)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the test data set is also 85%. Or: 85 % of the images were recognized correctly.\n",
    "It is unusual for networks to perform better or equally well on the test data set. This is an indication that you have not trained the model long enough.\n",
    "\n",
    "Next, you can look at which images the network has the most trouble with.\n",
    "The code in the next cell sorts the misrecognized images by their probability (this code is not necessarily easy to understand, but it is not essential to understanding neural networks either):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_classification = np.where(test_labels != np.argmax(test_y_hat, axis=1))[0]# which images were falsly classified\n",
    "len(false_classification) # this many images were falsly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-probabilities that the image was correctly classified\n",
    "probs = [] \n",
    "for image in false_classification:\n",
    "    probs.append(test_y_hat[image,test_labels[image]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sort the images based on the probabilities, the smaller the probability \n",
    "# the more certain the model was that the image is not in the right category.\n",
    "false_classification=false_classification[np.argsort(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what 10 images look like that are misclassified\n",
    "for i in range(10):\n",
    "    plt.imshow(test_images[false_classification[i]].reshape([28, 28]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(\n",
    "        \"Predicted Label: %s, Correct Label %s\"\n",
    "        % (\n",
    "            np.argmax(test_y_hat, axis=1)[false_classification[i]],\n",
    "            test_labels[false_classification[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some images, you can clearly see why they were categorized incorrectly. With others, however, it is quite easy for the human eye to recognize the correct digit, but the network has problems with it.\n",
    "\n",
    "# Exercise\n",
    "\n",
    "Rewrite the above code so that both Test Accuracy and Test Loss are calculated and output after each epoch. This should happen in addition to the training Accuracy/Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "W, b = init_weights(784, 300, 10)\n",
    "loss= []\n",
    "EPOCHS= 25 # how often the data is passed through the network\n",
    "for i in range(EPOCHS):\n",
    "    z_1, a_1, y_hat = forward_pass(W, b,train_images)\n",
    "    loss= calc_loss(y_hat, train_targets) / y_hat.shape[0]\n",
    "    acc = accuracy(train_labels, y_hat)\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss, acc)\n",
    "    )\n",
    "    \n",
    "    # backpropagation\n",
    "    grad_W, grad_b = back_prop(train_images, z_1, a_1, y_hat, train_targets)\n",
    "    # with the gradients we update the weights\n",
    "    W, b = update(W, b, grad_W, grad_b, lr = 0.3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
