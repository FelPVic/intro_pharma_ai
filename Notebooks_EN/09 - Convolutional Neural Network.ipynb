{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This week we are looking at convolutional neural networks. Convolutional neural networks (CNNs) are used primarily, but not exclusively, for image recognition.\n",
    "\n",
    "Unlike the neural networks we have seen so far, CNNs can read images as a matrix. This means that local context is not lost by `flattening` the image.  \n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*h01T_cugn22R2zbKw5a8hA.gif)\n",
    "\n",
    "<centering><h7> Otavio Good. 2017 \"A Visual and Intuitive Understanding of Deep Learning\" *O'Reilly AI Conference* </h7></centering>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, reload the training data and convert it to a `tensor`. In total there are 60,000 images with 784 pixels + a column for the labels of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('https://uni-muenster.sciebo.de/s/xSU1IKM6ui4WKAV/download', delimiter=',', header=None).values\n",
    "\n",
    "train_x = torch.tensor(min_max(train_data[:,1:]), dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data[:,0], dtype=torch.long)\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have always input images as 1D vectors to our neural network. But this time we want to use the 2D structure. For this we have to make a matrix of size `28 x 28` out of a vector of length `784`.\n",
    "\n",
    "For this we can use the function `vector.view(28,28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0,:].view(28,28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at this picture, but we can't see much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x[0,:].view(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with `matplotlib` we can represent arrays as an image. Here `cmap = \"gray\"` specifies that we want our color spectrum to be black and white only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0,:].view(28,28), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one image in the correct format so far, to do this for all images we can also use `.view()`. The `tensor` we created before had the format `(height,width)`. To be able to convert all images, we need to add an additional dimension to the `tensor`.  The new `tensor` should have the following dimensions: `(number of images, height, width)`. So we have a total of three dimensions.\n",
    "\n",
    "However, PyTorch would throw a wrench in our plans here. Because PyTorch can work with black and white (b/w) as well as with colored images. In PyTorch, colored images are represented by three matrices. One for red, one for green and one for blue. These are also called channels. A colored image would have the dimensions `(3, height, width)` in PyTorch. So the dimension we just used for `number of images` is occupied by the `number of channels`.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "\n",
    "<center><h7>Source: Mathanraj Sharma, 2019 </h7></center>\n",
    "\n",
    "PyTorch expects this \"channel dimension\" also for b/w images. \n",
    "Therefore we represent a b/w image as follows: `(1, height, width)`. \n",
    "\n",
    "It follows that all images from the MNIST dataset must match this format: `(number of images, 1, height, width)`. So in total our input `tensor` has 4 dimensions.\n",
    "\n",
    "\n",
    "\n",
    "Convert `train_x` to this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(_____,1,____,____)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "train_x = train_x.view(60000,1,28,28)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now converted all images to the format `(1,28,28)`.\n",
    "You can still display images with `plt.imshow`.\n",
    "\n",
    "Notice how the `tensor` is now indexed. `[0,0,:,:]`. We select the first image and also the first and only channel. We also select the entire height and width to display the image completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_x[0,0,:,:], cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the last notebook you can use a `DataLoader`. For this you have to create a PyTorch dataset first. With `next(iter())` you can output the first minibatch of the `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data.TensorDataset(_____, ____)\n",
    "train_loader = data.DataLoader(______, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "torch_train = data.TensorDataset(train_x,train_y)\n",
    "train_loader = data.DataLoader(torch_train, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `batch_x` has the dimensions `[32, 1, 28, 28]`. So `32` images, the size of our batch, `1` channel, `28` pixels in height and `28` in width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNNs in PyTorch.\n",
    "\n",
    "\n",
    "So far we have our data in the right format, now it is time to create a `CNN` in PyTorch. Just as there are `nn.Linear` layers in PyTorch, there are also Convolutional Layers in the `nn` module.\n",
    "\n",
    "`nn.Conv2d()` is one such layer. Before we use it, we briefly discuss the most important parameters.\n",
    "\n",
    "- `in_channels` the number of channels the image has before convolution .\n",
    "- `out_channels` how many channels the image should have after convolution. Or how many filters we run over the image.\n",
    "- `kernel_size` how big the kernel is, so the height/width in pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the size of the minibatch has changed. We still have 32 images, but as indicated, we now have 3 channels. The height and width of our image have also changed. We have lost 2 pixels per dimension. This is due to the way the convolution works.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*L4T6IXRalWoseBncjRr4wQ@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "\n",
    "Here is an example of why a kernel size of 3 makes our output image two pixels smaller. On the left is the input image and on the right is the output image. Since we can't push the kernel over the edge of the image, we \"lose\" the outer edge of the image.\n",
    "\n",
    "To prevent this information from being lost, we can *pad* the image. This way we enlarge the image, for example with pixels that have the value zero.\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "By padding, the kernel can be pushed once over the entire image.\n",
    "We can also specify the width of the padding as a parameter in `Conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1)\n",
    "out = conv1(batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the padding the image does not shrink anymore. Since we now have `3` channels, we can still display the image using `plt.imshow`.  To do this, we need to select an image from the minibatch and use the `detach()` command to remove the gradients stored by `autograd`.\n",
    "\n",
    "*An image like this can only be used as an example to illustrate the transformation. The actual colors and intensities are irrelevant here, since these are arbitrarily set by the network.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(min_max(out.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still see a 5, but this time in color. As described above, the colors can not be used for interpretation. They are only used to show the diversification of the input. \n",
    "\n",
    "The second new layer you will use today is `nn.MaxPool2d()`. \n",
    "\n",
    "This layer is called the **Pooling** layer.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)\n",
    "<center><h7>Source: Arden Dertat, 2017 </h7></center>\n",
    "\n",
    "Pooling layers leads to a deliberate reduction in image size deeper in the network. This means that fewer parameters (weights) are needed, which results in our networks training faster. When you look at an image (larger than 28 x 28 pixels), you don't recognize each pixel individually, but pixels in a certain proximity merge together. Pooling works in a similar way. Here, multiple pixels are combined using the maximum value.\n",
    "Fewer parameters also mean a lower probability of overfitting. \n",
    "\n",
    "The most commonly used pooling layer is the max pooling layer. Here the largest value in the region is chosen as the new value for the output. There are, of course, a variety of other [pooling layers](https://pytorch.org/docs/stable/nn.html#pooling-layers).\n",
    "In addition to the kernel size, the size of the square we want to pool, this time we also specify the `stride`. The stride determines by how many pixels we move the pooling kernel. \n",
    "\n",
    "![](https://epynn.net/_images/pool-01.svg)\n",
    "<center><h7>Source: Florian Malard and Stéphanie Olivier-Van Stichele - EPyNN </h7></center>\n",
    "\n",
    "\n",
    "[Here](https://ezyang.github.io/convolution-visualizer/index.html) is a website with which you can visualize the effect of different parameters on the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the output of the 2DConv (`out`) as input for the pooling layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out2 = pool1(____)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "out2 = pool1(out)\n",
    "```\n",
    "</details>\n",
    "\n",
    "Since the number of channels has not changed, we can still visualize this image.\n",
    "We can see that the image has shrunk, yet you can still see the 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(min_max(out2.detach().numpy()[0].transpose((1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `nn.Sequential` you can also write several convolution/pooling layers in a row. Important, we also need a nonlinear activation function again, this is normally inserted after the convolution.\n",
    "\n",
    "Fill in the missing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(in_channels=_, out_channels=3, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "                   nn.Conv2d(in_channels= _ , out_channels=6, kernel_size=3, padding =1),\n",
    "                   nn.______,\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2),\n",
    "                   nn.Conv2d(in_channels= 3 , out_channels=6, kernel_size=3, padding =1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass `batch_x` through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this output is not yet suitable for predictions. For this, we have to feed the images back into a conventional neural network. However, these only accept inputs in the form of vectors. So we convert each image back into a vector. \n",
    "\n",
    "The output `tensor` has the `shape` `[32, 6, 7, 7]` and should become a `tensor` of size `[32, 6 x 7 x 7]` = `[32, 294]`.\n",
    "\n",
    "For this we can use the layer `nn.Flatten(starting_dim)`. Here we only have to define the parameter `starting_dim`. This determines from which dimension we merge the dimensions. Since we want a separate vector for each image, we use `starting_dim = 1`. With `cnn.add_module()` we can add additional layers to our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the batch is now `(32,294)`. `32` is still the number of images in the batch (dimension 0), but our second dimension is now `294`. This means that each image in the batch has a vector associated with it. Now we can also add a traditional linear layer. However, before the layer `nn.Linear` we add an additional BatchNorm and a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", nn.BatchNorm1d(____))\n",
    "cnn.add_module(\"dp\", nn._________(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(____,___))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "cnn.add_module(\"bn\", nn.BatchNorm1d(294))\n",
    "cnn.add_module(\"dp\", nn.Dropout(0.2))\n",
    "cnn.add_module(\"fc\", nn.Linear(294,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can set the loss function and optimizer. Using PyTorch's `loaders` and the `nn` module, you can copy the same `for-loop` from last notebook without change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "update =  torch.optim.Adam(_____________, lr =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution:</b></summary>\n",
    "    \n",
    "```python \n",
    "loss_funktion = nn.CrossEntropyLoss()\n",
    "update =  torch.optim.Adam(cnn.parameters(), lr =0.001)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] \n",
    "    cnn.train() \n",
    "    for minibatch in train_loader: \n",
    "        images, labels = minibatch\n",
    "\n",
    "        update.zero_grad()\n",
    "        output = cnn(images) \n",
    "        loss   = loss_funktion(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        update.step()\n",
    "    cnn.eval()    \n",
    "    output = cnn(train_x)\n",
    "    train_acc=((output.max(dim=1)[1]==train_y).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we evaluate the network on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('https://uni-muenster.sciebo.de/s/fByBt5wd24chROg/download', delimiter=',', header = None).values\n",
    "test_x = torch.tensor(min_max(test_data[:,1:]), dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data[:,0], dtype=torch.long)\n",
    "test_x = test_x.reshape(test_x.shape[0],1,28,28)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn(test_x)\n",
    "acc=((output.max(dim=1)[1]==test_y).sum()/float(output.shape[0])).item()\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise\n",
    "\n",
    "Again we use the toxicity data for the exercise task. However, this time the molecules are not in SMILES format, but the structures are stored as an image. **You will again predict the toxicity, but this time based on the image**. \n",
    "\n",
    "In fact, this has [already been attempted](https://www.sciencedirect.com/science/article/abs/pii/S0169743919303417). \n",
    "\n",
    "The images consist of `64 x 64` pixels. You will see that this is barely sufficient to see the molecular structure.  However, we are bound to the memory space provided by the university.\n",
    "And a higher resolution does not change the underlying task.\n",
    "\n",
    "## Restart the kernel before beginning the exercise!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, divide the data into training set and test set. Important: the last column contains the information about the toxicity. So this is our variable to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_img_data = torch.tensor(pd.read_csv('https://uni-muenster.sciebo.de/s/oW8odaLJCgqycS3/download',\n",
    "                                        delimiter = ',', \n",
    "                                        header = None).values,\n",
    "                            dtype = torch.float32)\n",
    "\n",
    "train, test=train_test_split(_______________,______________,_________, random_state=1234)\n",
    "\n",
    "train_x = train[______]\n",
    "train_y = train[______]\n",
    "test_x = test[______]\n",
    "test_y = test[______]\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how pixelated the images look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[10,:].view(64,64), cmap= \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the test and training set. Remember that the dimensions should look like this. `Number of Images, Number of Channels, Height, Width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.view(__________________________)\n",
    "test_x = test_x.view(___________________________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = data.TensorDataset(______________________)\n",
    "train_loader = data.DataLoader(__________________, batch_size=32)\n",
    "\n",
    "batch_x, batch_y =next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done everything right so far, `batch_x` should have the dimensions `[32, 1, 64, 64]` and `batch_y` should have the dimensions `[32]`. Add at least 2 more convolution layers to the network. Make sure you also use pooling layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = nn.Sequential(nn.Conv2d(in_channels= , out_channels , kernel_size , padding ),\n",
    "                   \n",
    "                   \n",
    "                    \n",
    "                   \n",
    "                   \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a `flatten` layer. From which dimension do we start to add the values together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"flatten\",nn.Flatten(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last add a `BatchNorm`, `Dropout` and `Linear` layer. Make sure you use the correct input/output dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add_module(\"bn\", _______________)\n",
    "cnn.add_module(\"dp\", _______________)\n",
    "cnn.add_module(\"fc\", _______________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(batch_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` should now be `[32, 1]`. Fill in the rest of the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funktion = ________________________\n",
    "updaten =  torch.optim.Adam(_______________, lr =0.0003)\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] \n",
    "    \n",
    "    ___.train() \n",
    "    for minibatch in train_loader: \n",
    "        images, labels = minibatch \n",
    "        ________.zero_grad()\n",
    "        output = cnn(_______) \n",
    "        loss   = loss_funktion(output.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        updaten.step()\n",
    "    ___.eval()    \n",
    "    \n",
    "    # Training Evaluation\n",
    "    output = cnn(train_x)\n",
    "    train_acc = torch.sum((output>0).squeeze().int() == train_y)/train_y.shape[0]\n",
    "    # Test Evaluation\n",
    "    output = cnn(test_x)\n",
    "    loss   = loss_funktion(output.squeeze(), test_y)\n",
    "    test_acc = torch.sum((output>0).squeeze().int() == test_y)/test_y.shape[0]\n",
    "    \n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f | Test Loss: %.2f  Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), train_acc, loss.item(),test_acc )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this works only moderately. It definitely works better with fingerprints.\n",
    "Generally, it is more difficult to train CNNs than simpler neural networks. \n",
    "Furthermore, drawing molecules is very inefficient compared to calculation of SMILES or fingerprints/descriptors. \n",
    "\n",
    "In our case, one could argue that our model could also learn better if we had larger and more colorful images. This is probably true. But even in the above paper, CNNs simply could not beat networks with fingerprints.\n",
    "It can be said that images are not an adequate representation of molecules. At least not for machine learning.\n",
    "\n",
    "This is not to say that it cannot be useful to train CNNs on images of molecules. \n",
    "For example, networks that recognize structures and output the appropriate SMILES. This can be used to quickly search patents and chemical publications.\n",
    "\n",
    "\n",
    "For example here:\n",
    "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00538-8"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
