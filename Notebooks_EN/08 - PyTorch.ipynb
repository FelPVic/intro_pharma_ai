{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://venturebeat.com/wp-content/uploads/2019/06/pytorch-e1576624094357.jpg?w=1200&strip=all\" style=\"width: 800px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### In this lesson you'll learn:\n",
    "* how to program a simple neural net using PyTorch.\n",
    "* how to implement more advanced layers in your neural net (Dropout, Batchnorm).\n",
    "* about more advanced optimisation (Momentum, adam).\n",
    "---\n",
    "\n",
    "Last week you programmed a simple neural network yourself. As mentioned earlier, it is not necessary to program every net yourself. Certain software packages take care of many of the inconveniences of creating and training nets \"by hand\".\n",
    "\n",
    "Essentially, there are two libaries that can be used: PyTorch and TensorFlow. TensorFlow is developed by Google and is the more popular choice, especially in industry. PyTorch, on the other hand, is mostly used in the scientific world. Basically, PyTorch is considered the easier framework to learn and is a bit more user-friendly overall. \n",
    "\n",
    "While there used to be major differences, today the two libraries are becoming more and more similar in functionality.\n",
    "\n",
    "Finally, there is Keras and PyTorch Lightning. Both aim to make neural network creation even easier. Keras uses TensorFlow in the background, but makes it easier to train networks, especially for beginners. The same is true for PyTorch Lightning and PyTorch.\n",
    "\n",
    "In cheminformatics, however, PyTorch is a good choice, since special libraries such as for Graph Neural Networks exist or existed only for PyTorch.\n",
    "\n",
    "\n",
    "An essential part of PyTorch is **autograd**. Autograd is a library that, as the name suggests, can compute and collect the gradients automatically. So you don't have to calculate the gradients yourself.\n",
    "Also, there are many functions, like activation functions or linear transformations, that are already implemented in PyTorch. \n",
    "\n",
    "*TensorFlow has these functionalities as well, of course.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "While you have been working with `numpy` arrays so far, today we will use `tensors`, more precisely PyTorch `tensors`. \n",
    "\n",
    "**What is the difference?**\n",
    "\n",
    "First of all, none. Arrays and tensors are similar in many ways. Both store numbers/values in a structured form. So in NumPy you can store matrices in a 2D array, but you can also store the same matrix in a 2D tensor.\n",
    "Also, tensors can be converted to arrays and arrays can be converted to tensors.\n",
    "\n",
    "The difference between the two \"storage options\" is that PyTorch tensors were developed by PyTorch. And NumPy arrays were developed by the developers of NumPy. \n",
    "Many features that NumPy offers are also available from PyTorch for their tensors (but may be called differently). \n",
    "PyTorch developed its tensors to perform mathematical operations faster. In addition, tensors can also be \"loaded\" onto the graphics card, which increases the speed of operations many times over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations with `tensors` are almost identical to calculations with `np.arrays`. But the functions can have different names. For example `torch.mm()` is the function for matrix multiplication and `.t()` is the transpose of a matrix. Similar to the function `np.array()` to create arrays, `torch.tensor()` creates tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # loads PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1,2,3],\n",
    "                [4,5,6]])\n",
    "\n",
    "W = torch.tensor([[8,9,10],\n",
    "                 [11,12,313]])\n",
    "\n",
    "b = torch.tensor([1,2])\n",
    "\n",
    "torch.mm(X,W.t())+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the linear transformation, which you also already know.<br>\n",
    "However, PyTorch simplifies this step. \n",
    "In PyTorch there is a module called `nn`, this contains many functions that are helpful in creating neural networks.\n",
    "\n",
    "We can load the module `nn` with `from torch import nn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submodule `nn` provides among others the function `nn.Linear`. It performs the linear transformation $xW^T +b $.\n",
    "As input the function takes:\n",
    "\n",
    "\n",
    "* `in_features` the number of features the input has before the transformation, or the size of the input layers. Yesterday the images had 784 pixels, so 784 features.\n",
    "* `out_features` the number of features the input should have after the transformation. So `out_features` defines the size of the hidden layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn`t the input for the layers missing?\n",
    "\n",
    "That's right, until now you haven't performed a linear transformation either, but only created a variable `layer_1`. This can then perform the linear transformation for us.\n",
    "\n",
    "---\n",
    "*Technically `nn.Linear` is not a function, but a `class`. Classes are special Python objects. How exactly classes work is not relevant for this course. What is important to understand is that*\n",
    "\n",
    "```py\n",
    "layer_1 = nn.Linear(in_features = 784, out_features=300, bias=True)\n",
    "```\n",
    "*creates an object `layer_1` which belongs to the class `nn.Linear`. Each class in Python can have its own functions. For example, most `nn` classes have a `forward` function that executes a particular forward pass.*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical feature of `nn` layers is that the weights of these layers are automatically initialized by PyTorch. This already saves you a little work.\n",
    "The weights $W$ of these layers can also be viewed.\n",
    "\n",
    "For this you use `list(layer_1.parameters())[0]`.\n",
    "If you want to know the exact size of the weight matrix, you can use `.shape` like NumPy: `list(layer_1.parameters())[0].shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(layer_1.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the weight matrix has the same size as the matrix of the last week. You can also see that the matrix actually contains weights.\n",
    "All you need now is an input (images) that you want to change with this linear transformation. \n",
    "This is done by loading the training dataset from last week with `numpy`.\n",
    "\n",
    "Additionally, the images need to be transformed into a tensor. For this you use `torch.tensor()`.\n",
    "\n",
    "\n",
    "Of course you have to scale the data again. For this you use the min-max scaler.\n",
    "In PyTorch, you have to pay more attention to the data types. \n",
    "Therefore we define the datatype `dtype`. The datatype for our images is `float32`. You know `float` from yesterday, the `32` defines how exact this number can be.\n",
    "`long` may not mean anything to you, but it simply denotes `integers`.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Only for the particularly interested:</strong></summary>\n",
    "\n",
    "In the last notebook we discussed why we initialize the weight matrix in this particular way.\n",
    "In fact, in TensorFlow, the weight matrices are created so that matrix multiplication can be performed without `transpose`.\n",
    "\n",
    "    \n",
    "Here is the description of the code for the forward pass in TensorFlow.\n",
    "    \n",
    "\n",
    "  `Dense` implements the operation:\n",
    "  `output = activation(dot(input, kernel) + bias)`\n",
    "  where `activation` is the element-wise activation function\n",
    "  passed as the `activation` argument, `kernel` is a weights matrix\n",
    "  created by the layer, and `bias` is a bias vector created by the layer\n",
    "  (only applicable if `use_bias` is `True`). These are all attributes of\n",
    "  `Dense`.\n",
    "\n",
    "PyTorch's description can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "    \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "train_data = np.genfromtxt('https://uni-muenster.sciebo.de/s/xSU1IKM6ui4WKAV/download', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "train_images = min_max(train_data[:,1:])\n",
    "train_images = torch.tensor(train_images, dtype = torch.float32)\n",
    "train_labels=torch.tensor(train_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "\n",
    "test_data = np.genfromtxt('https://uni-muenster.sciebo.de/s/fByBt5wd24chROg/download', delimiter=',', skip_header =False) #genfromtxt reads .txt files if we chose delimiter =\",\" the function can read also .csv files  (comma seperated values)\n",
    "\n",
    "test_images = min_max(test_data[:,1:])\n",
    "test_images = torch.tensor(test_images, dtype = torch.float32)\n",
    "test_labels=torch.tensor(test_data[:,0].astype(int), dtype = torch.long) \n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 60000 images with 784 pixels each.\n",
    "You can now use these as input for the linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1=layer_1(train_images)\n",
    "print(z_1)\n",
    "z_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `layer_1` returns the output (`z_1`). This has the shape `[60000,300]`. So still 60000 images, but this time each has only 300 features (size of the hidden layer). Just like it was defined when `layer_1` was created.\n",
    "\n",
    "\n",
    "What you should notice is the `grad_fn=<AddmmBackward>` at the end of the `z_1` tensor. You can see that *autograd* has captured the gradients for this transformation. You can also see that we have performed a matrix multiplication `mm` and an addition `Add`. The last performed transformation of the tensor is always shown.\n",
    "\n",
    "What you are missing now is the activation function. PyTorch can help here as well. \n",
    "The `nn` library of PyTorch has a submodule `functional`. Here are many additional mathematical functions included, among others the `relu` and `sigmoid` functions. Since the `relu` function gives even better results in practice than the `sigmoid` function, we will use it now.\n",
    "\n",
    "`functional` can be imported as follows: `from torch.nn import functional as F`. We rename `functional` to `F`, sort of a standard when working with PyTorch.\n",
    "\n",
    "`F.relu()` can now be used to apply the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "a_1 = F.relu(z_1)\n",
    "print(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare `a_1` with `z_1`, you can see that all values that were negative before became zero and all values that were positive are unchanged.\n",
    "Also you can see in the `grad_fn` that a ReLU was applied. This was also recorded by *autograd*. \n",
    "\n",
    "The first part of the forward pass is already done. \n",
    "For the second step, we can simply create another layer that gets `a_1` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = nn.Linear(300,10) # 10 is the number of out_features, since we have 10 digits.\n",
    "z_2=layer_2(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you need an activation function, but this time the `softmax` function to get the probabilities.\n",
    "`nn.functional` has also a `softmax()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = F.softmax(z_2,dim=1) # the dim parameter defines whether the softmax function is applied over columns or rows.\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch you can also combine different layers. With `nn.Sequential` you can write the linear transformation and the activation function directly one after the other. The input is automatically passed through each of the layers.\n",
    "This makes code clearer and easier to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(784,300), \n",
    "                         nn.ReLU(), \n",
    "                         nn.Linear(300,10))\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a network with a hidden layer has been created. What you should notice is that instead of `F.relu` `nn.ReLU` has been used. If a `relu` function is to be used inside `Sequential()`, you must always use `nn.ReLU`. \n",
    "\n",
    "The `network` can now classify the images:\n",
    "`network(input)` can be used to pass the input, e.g. our images, through the network.\n",
    "From the tensor size of the output you can see that in the end there are actually 60000 images with 10 features each (the 10 digits) as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(train_images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another change is that you no longer use the last activation function. PyTorch chooses it automatically. The decision of which activation function to use in the last layer depends on the choice of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "\n",
    "`nn` can also help with the loss function. The most common loss functions are already included in PyTorch.\n",
    "You can simply create a new variable and assign the `nn.CrossEntropyLoss()` function to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `loss_function` can now calculate the loss by automatically applying the softmax function.\n",
    "To do this, you only have to enter `y_hat` and the `train_labels` into the function. Here you can see another advantage of PyTorch: you don't have to `one-hot` encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(output, train_labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The last step is to perform a backpropagation. Thanks to *autograd* this is easily possible with the command `loss.backward()`. It calculates the gradients for all weight matrices.\n",
    "\n",
    "After that you only have to update the weight matrices. As you can imagine, PyTorch takes care of this task as well.\n",
    "PyTorch even provides a variety of different algorithms that update the weights in different ways.\n",
    "\n",
    "To update the weights, a new module of PyTorch is needed.\n",
    "For this one loads `from torch import optim`. `optim` contains functions that optimize the net for us - i.e. updating the weights.\n",
    "\n",
    "Similar to the loss function, you can simply create a variable and assign an update function to it. \n",
    "You can now use the function `optim.SGD()` to update the weights. SGD = Stochastic Gradient Descent.  In the function itself you define which parameters (weights) are to be changed. You also define the learning rate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # collects the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "update_weights=optim.SGD(net.parameters(), lr=0.01) \n",
    "# You define which parameters and with which learning rate these should be changed.\n",
    "\n",
    "update_weights.step()  # step() updates the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have everything you need to train a net.\n",
    "\n",
    "You can again use a `for-loop` to automate the training.\n",
    "\n",
    "You will notice that we also use `update.zero_grad()`. This function is used to clear the gradients from the previous update. If this is not done, the optimizer would constantly sum all gradients from all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define network, loss function and update algorithm\n",
    "net = nn.Sequential(nn.Linear(784,300), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(300,10))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "update = optim.SGD(net.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    update.zero_grad()\n",
    "    output = net(train_images) # forward propagation\n",
    "    \n",
    "    loss   = loss_function(output, train_labels)\n",
    "    loss.backward()\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i, \n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    update.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that you can train a neural network with much less effort. You can also add a second or third hidden layer to your model without much effort.\n",
    "Just add an `nn.ReLU` and an `nn.Linear` layer to `Sequential`, and *autograd* can calculate the gradients for these layers as well. Everything else remains the same. Just remember that the dimensions must fit from one layer to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define network, loss function and update algorithm\n",
    "net = nn.Sequential(nn.Linear(784,300), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(300,300),# <----- EXTRA LAYER\n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(300,10))\n",
    "print(net)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "update = optim.SGD(net.parameters(), lr=0.3)\n",
    "EPOCHS = 50\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    update.zero_grad()\n",
    "    output = net(train_images) # forward propagation\n",
    "    \n",
    "    loss   = loss_function(output, train_labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (loss.item(), acc)\n",
    "    )\n",
    "    \n",
    "    update.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we are using Stochastic Gradient Descent as an optimizer (to update the weights). So far, we have only talked about Gradient Descent.\n",
    "In fact, as explained in the lecture, Gradient Descent is actually no longer used, but Stochastic Gradient Descent is used as an alternative.\n",
    "\n",
    "The difference:<br>\n",
    "In Stochastic Gradient Descent, the data set is not sent through the network all at once, but the dataset is passed through the network in smaller parts (**minibatches**).<br>\n",
    "In this dataset, there are 60000 images in total. With Gradient Descent, the forward pass is done simultaneously with 60000 images, and for the 60000 images the loss is calculated simultaneously. After that, the weights are updated **once**.\n",
    "Then the step is repeated.\n",
    "\n",
    "It would be more efficient if an update would not take place after every 60000 images, but after 200 or even only 100 images, so that the network can learn much faster.\n",
    "This is exactly what the Stochastic Gradient Descent does.  Not all images, but only e.g. 32 images are sent through the network at once. For these 32 images the loss is then calculated and the updates are performed.\n",
    "Then this step is repeated, but this time with 32 new images. In this way, the weights can be updated much more frequently within an epoch.\n",
    "\n",
    "The batch size specifies how large a minibatch (the small portion of data that is passed through the network) should be, and can also affect the performance of the model.\n",
    "<center>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcT0TVrYkk0A0FfPvnzYTe747F0qPLG2rU2Bmg&usqp=CAU\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Source: Insu Han and Jongheon Jeong, http://alinlab.kaist.ac.kr/resource/Lec2_SGD.pdf</center></h8>\n",
    "\n",
    "\n",
    "Advantages:<br>\n",
    "* faster\n",
    "* needs less memory (on the graphics card)\n",
    "\n",
    "Disadvantages: <br>\n",
    "* cannot find the optimum (but can also be good to prevent overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of the Stochastic Gradient Descent, we first need to split the data into minibatches. Also for this we can use PyTorch, the corresponding functions are available in the submodule `torch.utils.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch.utils.data` there are two functions you need:\n",
    "\n",
    "* `data.TensorDataset(input,labels)` creates a PyTorch dataset from the data.\n",
    "* `data.DataLoader(Dataset, batch_size)` creates minibatches of the specified size from a PyTorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.TensorDataset(train_images, train_labels) \n",
    "# input are our tensors which contain the images and the labels\n",
    "loader = data.DataLoader(train_data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `loader` now contains 1875 minibatches, each with 32 images and their 32 labels. In the next cell you can see the content of the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring everything together, you need a second `for-loop` that selects the minibatches one by one within the first `for-loop` and passes them through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define network, loss function and update algorithm\n",
    "net = nn.Sequential(nn.Linear(784,300), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(300,300),\n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(300,10))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "update = optim.SGD(net.parameters(), lr=0.3)\n",
    "EPOCHS = 2\n",
    "\n",
    "## Trainings Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # this list stores the loss of each minibatch\n",
    "                   # with this we can calculate the average loss within the epoch\n",
    "    for minibatch in loader: # loop through all minibatches\n",
    "        images, labels = minibatch # minibatch is divided into images and labels\n",
    "        \n",
    "        update.zero_grad()\n",
    "        output = net(images) # forward propagation\n",
    "    \n",
    "        loss   = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        update.step()\n",
    "        \n",
    "    output = net(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    print(\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After only two epochs, the accuracy is much higher than ever before. A single epoch takes much longer compared to \"normal\" Gradient Descent, but the total training time is reduced.\n",
    "\n",
    "To calculate the Accuracy, after all the minibatches have passed through the network, the entire dataset is sent through the network again (without changing the weights). Accuracy is then calculated based on these predictions, The loss of the epoch is the average loss of the minibatches.\n",
    "\n",
    "Tip: If you want to copy/output the value of a tensor and not the tensor itself, you can use `x.item()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Layers\n",
    "\n",
    "\n",
    "In the following we will deal with new layers that are used in addition to linear layers.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is used during training to randomly *remove* individual neurons from the network. In other words, they are turned off for a short time. Mathematically, this means that their output is simply set to zero.\n",
    "Each time a batch passes through the network, the output of randomly selected neurons is set to zero. \n",
    "\n",
    "This random temporary *erasure* of neurons forces the network to not rely on individual neurons. Similar to Random Forest, where variables are randomly removed, Dropout prevents *overfitting* .\n",
    "However, during the validation of the network (validation or test set), no more neurons are removed and the dropout layer is skipped.\n",
    "\n",
    "How many neurons are removed from the network is another hyperparameter that, like the learning rate, can be chosen by you. The dropout is given as a percentage. So a dropout of `0.8` means that in this layer 80% of the neurons are removed or their output is set to zero. A default value for the dropout is `0.2`. Often a dropout layer is used directly after the input.\n",
    "\n",
    "In PyTorch, a dropout layer is defined as `nn.Dropout(0.2)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1235)\n",
    "\n",
    "example_x = torch.tensor([[1.,2.,3.,4.,5.]] )\n",
    "do = nn.Dropout(0.5)\n",
    "do(example_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the values were set to `0`, but the other values doubled. **Why did this happen?**\n",
    "\n",
    "This is because we train with dropout but evaluate without dropout. This means that with a dropout of `0.5` only half of the neurons are used. In the forward pass, the values are passed as a weighted sum. If half of the neurons have the value `0`, the sum is obviously much smaller than in a network where no dropout is used. In the course of training, the scale of the weights is adjusted to the expected scale of the inputs.\n",
    "\n",
    "Without Dropout: $$z = \\beta_0 + \\beta_11 + \\beta_22 +\\beta_33 +\\beta_44 +\\beta_55$$\n",
    "\n",
    "With Dropout: $$\\begin{align}z&= \\beta_0 + \\beta_10 + \\beta_22 +\\beta_30 +\\beta_44 +\\beta_50 \\\n",
    "&=\\beta_0 + \\beta_22+\\beta_44\\end{align}$$\n",
    "\n",
    "\n",
    "The sum $z$ with dropout is always smaller than the sum without dropout. By increasing the size of the forwarded inputs, we ensure that the weights do not adjust to the wrong scale of inputs.\n",
    "\n",
    "This is because in the evaluation, where no dropout is used, we suddenly have twice as many inputs. This discrepancy between training and evaluation is thus prevented.\n",
    "\n",
    "You can change a layer or a complete network from training to evaluation mode by using `network.eval()`. To put it in training mode, `.train()` is used. **Important**: By default every layer and every network is in `train()` mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do.eval()\n",
    "do(example_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `.eval()` mode the dropout is not applied.\n",
    "\n",
    "## Batchnorm\n",
    "\n",
    "\n",
    "Batchnorm layers are another commonly used layer in neural networks. As the name suggests, they normalize the batches, or more accurately, the activations of the minibatches.\n",
    "Recall that we scale our inputs. Batchnorm does the same thing, but in the net itself, so the values deeper in the net are also of roughly the same order of magnitude. Basically, the variables are normalized as follows:\n",
    "\n",
    "$$x_s = \\frac{x-\\bar{x}}{sd_x}$$\n",
    "\n",
    "Where $\\bar{x}$ is the mean and $sd_x$ is the standard deviation of $x$.<br><br>\n",
    "In a classical neural network, the activations of a layer are computed in two steps. First, the linear transformation is performed:\n",
    "\n",
    "$$Z = XW^T+b$$\n",
    "*Here $X$ is a minibatch, so for example only 32 images*.\n",
    "\n",
    "This is followed by a non-linear activation function:\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "With Batchnorm the values are normalized again before the activation function.\n",
    "\n",
    "$$Z_s = (\\frac{Z-\\bar{Z}}{sd_Z}) \\cdot \\gamma + \\beta $$\n",
    "\n",
    "This is done independently for each neuron. The mean $\\bar{Z}$ and the standard deviation $sd_Z$ are again calculated only per minibatch. What is new is the $\\gamma$ and $\\beta$. These are just two individual parameters that can shift and scale the normal distribution $N(0,1)$. These two parameters are also learnable, i.e. they are also changed during training.\n",
    "\n",
    "It is also important to note that during training, the average values of the minibatches are combined into an average value over all minibatches. This average is then used in the evaluation of the test set to normalize the minibatches.\n",
    "\n",
    "*Whether you apply the activation function first and then the batchnorm or vice versa is a question that no one can answer for you. There are pros and cons to both methods.*\n",
    "\n",
    "Complete the `layer_one`. The size of the hidden layers and the dropout is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(loader)) # here the first minibatch is chosen\n",
    "\n",
    "layer_one = nn.Sequential(nn.Linear(____,___),\n",
    "                         nn.BatchNorm1d(_____),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(____))\n",
    "layer_one(batch_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b></summary>\n",
    "\n",
    "```python\n",
    "layer_one = nn.Sequential(nn.Linear(784,300),\n",
    "                         nn.BatchNorm1d(300),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend the complete net from before.\n",
    "Important: Neither batchnorm nor dropout is used after the last linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net= nn.Sequential(nn.Linear(784,300), \n",
    "                   ______________,\n",
    "                   ______________,\n",
    "                   ______________,\n",
    "                   nn.Linear(300,300),\n",
    "                   ______________,\n",
    "                   ______________,\n",
    "                   ______________,\n",
    "                   nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b></summary>\n",
    "\n",
    "```python\n",
    "net= nn.Sequential(nn.Linear(784,300), \n",
    "                   nn.BatchNorm1d(300),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Dropout(0.2),\n",
    "                   nn.Linear(300,300),\n",
    "                   nn.BatchNorm1d(300),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Dropout(0.2),\n",
    "                   nn.Linear(300,10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "\n",
    "Optimizers determine how accurately the weights of the net are updated. So far you have always used the `SGD`.  In the PyTorch implementation of `SGD` there are more parameters for `SGD` that can make the training more effective. One of them is *momentum*. When you use momentum for training, the weights are not only updated according to the gradients of the current minibatch. The gradients of the previous minibatches also have an influence. The parameter `momentum` indicates how strong or weak the influence of the previous minibatches is.\n",
    "\n",
    "The momentum should optimize the loss in a straight line. As an example you can imagine a ball rolling down a hill. The longer this rolls in the same direction, the faster it becomes. And the faster it gets, the less influence small changes in direction triggered by changes in the terrain (gradients) have. \n",
    "\n",
    "So if the gradients have been pointing in the same direction for a while, a single minibatch should not change that direction all at once. \n",
    "\n",
    "One of the most commonly used algorithms for training neural nets is the \"ADAM\" algorithm. It combines many improvements from SGD, including a version of Momentum.  \n",
    "\n",
    "**ADAM does not always have to be better than SGD**.\n",
    "\n",
    "An overview of all available optimization algorithms can be found [here](https://pytorch.org/docs/stable/optim.html#algorithms). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "update = optim.Adam(net.parameters(), lr=0.1)\n",
    "EPOCHS = 10\n",
    "\n",
    "## Training Loop\n",
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # in this list we save the loss of each minibatch so we can\n",
    "                   # calculate the average loss at the end of the epoch\n",
    "    net.train()\n",
    "    for minibatch in loader: # loop through all minibatches\n",
    "        images, labels = minibatch # divide minibatches in labels and images\n",
    "        \n",
    "        update.zero_grad()\n",
    "        output = net(images) # forward propagation\n",
    "    \n",
    "        loss   = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        update.step()\n",
    "    net.eval()    \n",
    "    output = net(train_images)\n",
    "    acc=((output.max(dim=1)[1]==train_labels).sum()/float(output.shape[0])).item()\n",
    "    \n",
    "    print(i,\n",
    "        \"Training Loss: %.2f Training Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise\n",
    "\n",
    "For the exercise, you will train a network, but this time using the toxicity data from notebook 5.\n",
    "First, load all the required libraries and data again. This time, also use Batchnorm, Dropout and the ADAM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "if 'google.colab' in sys.modules: # checks whether the notebook runs on collab\n",
    "    !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/utils.py\n",
    "    !pip install rdkit==2022.3.4\n",
    "    %run utils.py\n",
    "else:\n",
    "    %run ../utils/utils.py # loads pre-written functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tox = pd.read_csv(\"https://raw.githubusercontent.com/filipsPL/tox21_dataset/master/compounds/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] # all columns except the first (index 0) are chosen\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you calculate the fingerprints. As in notebook 5, the function `get_fingerprints` is available for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data_tox)\n",
    "fps[\"activity\"] = data_tox.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use them in Pytorch, you need to convert both the fingerprints and the `acitivty` to `tensors`. Note that both are  in the DataFrame `fps`. \n",
    "\n",
    "`.values` converts a DataFrame into an `np.array`.\n",
    "\n",
    "Then the data is split into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = torch.tensor(___.values, dtype=torch.float32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(fps,test_size= 0.2 , train_size= 0.8, random_state=1234)\n",
    "\n",
    "\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use minibatches again. For this we still have to convert our training data into a `DataLoader`. Why only the training data? The use of minibatches is only relevant for training. As long as your computer is able to run the test dataset through the network all at once, we don`t need to split the test dataset into minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.TensorDataset(______, _____) # input are our tensors, for the fingerprints and the activities\n",
    "loader=data.DataLoader(train_data, batch_size = 32)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the net so that the input and output are the right size. So the length of the fingerprints and the number of classes we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net= nn.Sequential(nn.Linear(), \n",
    "                   nn.BatchNorm1d(),\n",
    "                   nn.ReLU(), \n",
    "                   nn.Dropout(),\n",
    "                   nn.Linear(),\n",
    "                   nn.BatchNorm1d(),\n",
    "                   nn.ReLU(), \n",
    "                   nn.Dropout(),\n",
    "                   nn.Linear())\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "update = ___________________, lr=0.1)    \n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, fill the `for loop`. \n",
    "\n",
    "`.squeeze` converts the `(n,1)` `output` tensor to a 1-dimensional `tensor` of length `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss_list = [] # in this list we save the loss of each minibatch \n",
    "    for minibatch in loader: # loop through all minibatches\n",
    "        update.__________\n",
    "        molecules, activity = minibatch # divide minibatches in labels and molecules\n",
    "        output = net(____________) # forward propagation\n",
    "        loss   = loss_function(output.squeeze(), ____________)\n",
    "        loss._______\n",
    "        loss_list.append(loss.item())\n",
    "        update.________\n",
    "    # here the accuracy for the testset is calculated\n",
    "    output = net(test_x)\n",
    "    acc = torch.sum((output>0).squeeze().int() == test_y)/float(test_y.shape[0])\n",
    "   \n",
    "    print(\n",
    "        \"Training Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list), acc.item())\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2063f454e5583264956edca724ed174a35400d49c5baf96fcf9ea99fcd5830b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
