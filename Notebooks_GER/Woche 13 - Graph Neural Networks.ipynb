{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5efb70",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "\n",
    "---\n",
    "**Lernziele**\n",
    "* Sie verstehen, wie Moleküle als Graphen für Computer dargestellt werden können. \n",
    "* Sie verstehen, wie Graph Neural Networks funktionieren.\n",
    "* Sie sind in der Lage, ein Graph Neural Network als Pytorch Klasse zu schreiben.\n",
    "---\n",
    "\n",
    "\n",
    "Graph Neural Networks sind noch eine relativ neue Methode. Intuitiv lassem sich Moleküle sehr gut als (mathematischer) Graph dargestellt. Die Bindungen eines Molekül entsprechen den Kanten des Graphen und die Atome den Knoten. <br> Für den Computer sind Graphen jedoch nicht so einfach zu lesen wie z. B. ein Smiles `string`. Für Graph Neurale Netzwerke stellen die Moleküle durch mindestens zwei Matrizen dar. Die eine ist die Adjacencymatrix, die die Verbindungen zwischen den Atome (d. h. die Bindungen) darstellt. Die andere Matrix ist die Featurematrix. Hier können Informationen zu einzelnen Atomen gespeichert werden.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Jorge_Galvez2/publication/236018587/figure/fig1/AS:299800013623305@1448489301609/The-chemical-graph-and-adjacency-matrix-of-the-isopentane.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "<h8><center>Galvez et. al. 2010</center></h8><br><br>\n",
    "\n",
    "Für das heutige Beispiel benutzen wie nochmal die Daten aus der Tox21 Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from os.path import exists\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install rdkit==2022.3.4\n",
    "    if exists(\"utils.py\") == False:\n",
    "        !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/onehotencoder.py\n",
    "    %run onehotencoder.py\n",
    "\n",
    "else:\n",
    "    %run ../utils/onehotencoder.py\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "np.set_printoptions(linewidth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten\n",
    "data_tox = pd.read_csv(\"https://raw.githubusercontent.com/filipsPL/tox21_dataset/master/compounds/sr-mmp.tab\", sep = \"\\t\")\n",
    "data_tox = data_tox.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data_tox.columns = [\"smiles\", \"activity\"]\n",
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321a720",
   "metadata": {},
   "source": [
    "## Adjacency Matrix und One-Hot Encoded Feature Matrix\n",
    "\n",
    "Leider können wir dieses Mal nicht viel mit den Smiles anfangen. Dafür gibt es in RDKit Funktionen, die es uns erleichtern mit Graphen zu arbeiten. Zum Beispiel gibt es eien Funktion, die Adjacencymatrix für Moleküle erstellt. Wir haben diese bereits oben importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [Chem.MolFromSmiles(x) for x in data_tox['smiles']]\n",
    "A = [GetAdjacencyMatrix(x) for x in mols]\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0e03f",
   "metadata": {},
   "source": [
    "\n",
    "Als Features für die Atome benutzen wir nur den Atomtype. Diese werdem wir auch One-Hot kodieren.\n",
    "\n",
    "Für die One-Hot Kodierung der Atome verwenden wir die bereits geschriebene Funktion `onehotencode()`.\n",
    "Ähnlich wie bei der Kodierung von Smilestokens in RNNs wird hier nur der Atomtyp eines jeden Atoms erfasst. Diese Atome werden dann als One-Hot kodierte Vektoren dargestellt.\n",
    "Pro Molekül werden diese Vektoren miteinander kombiniert, um eine Matrix zu bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fb9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = onehotencode(_____)\n",
    "feat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac70ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Lösung:</strong></summary>\n",
    "\n",
    "```python\n",
    "feat = onehotencode(mols)\n",
    "feat[1]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926e98c",
   "metadata": {},
   "source": [
    "Oben können Sie sehen, wie eine Featurematrix für ein Molekül aussieht.\n",
    "Wenn wir uns die `.shape` ansehen, können wir sehen, dass dieses Molekül aus `29` Atomen besteht. Die Anzahl der Spalten lässt uns wissen, dass es insgesamt 25 Atomtypen im Datensetz gibt. Die Anzahl der Spalten (Anzahl der Features) muss für alle Moleküle gleich sein, sonst können wir die Moleküle nicht durch das Netzwerk führen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4961d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd903",
   "metadata": {},
   "source": [
    "Sie haben vielleicht bemerkt, dass auf der Diagonalen der Adjacencymatrix noch Nullen stehen. Bei einer Graph Convolution sollen aber nicht nur die Features der Nachbaratome, sondern auch die des Zentralatoms in die Berechnung einbezogen werden. Hierfür werden Einsen auf der Diagonalen der Adjacencymatrix benötigt. \n",
    "Mit der Funktion `np.fill_diagonal(matrix, value)` können Sie die Werte der Diagonalen einer Matrix ändern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for matrix in A:\n",
    "    np.fill_diagonal(matrix, 1)\n",
    "print(A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ff2ee",
   "metadata": {},
   "source": [
    "## Graph Convolution\n",
    "\n",
    "Nun wollen wir die Informationen der Knoten an die Nachbarknoten entlang der Kanten weitergeben. Das wird (Graph-)Convolution bezeichnet. Dazu werden bei der Forward Propagation diese mathematischen Operationen durchgeführt: $$\\hat{X} = \\hat{D}^{-1}\\hat{A}XW$$.\n",
    "\n",
    "Dabei ist $\\hat{A}$ unsere Adjazcenymatrix, mit Einsen auf der Diagonalen. $X$ ist die Featurematrix und $W$ sind die Weights, die Pytorch initialisiert. Was noch fehlt, ist $D$, die $D$egreematrix. Diese Matrix enthält die Anzahl der Bindungen, die jedes Atom im Molekül hat. Diese Werte werden auf der Diagonalen der Degreematrix platziert.\n",
    "Die Anzahl der Verbindungen eines jeden Moleküls lässt sich leicht aus der Adjacency Matrix berechnen. Dazu muss man die Summe über die  Zeilen oder Spalten der Matrix $\\hat{A}$ berechnen. Diese Summe ergibt den Degree eines Atoms. Um genau zu sein, ist es der Degree plus eins, da man die Diagonale der Adjacencymatrix bereits mit Einsen gefüllt hat.\n",
    "Um die Degree-Matrix zu erstellen, müssen Sie die Summen der Spalten in jeder Adjacenymatrix berechnen und diese auf die Diagonale einer neuen Matrix setzen.\n",
    "\n",
    "\n",
    "Dieser Vorgang ist mit `numpy` relativ einfach durchzuführen. `np.sum(A[i], axis=0)` berechnet die Summe pro Spalte und `np.diag()` erzeugt eine Matrix aus einem 1D-Array mit den Werten des 1D-Arrays auf der Diagonalen. Sie können die beiden Funktionen in Kombination verwenden, um die Degreematrizen zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bef88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D =[]\n",
    "for matrix in A:\n",
    "    D.append(np.diag(np.sum(matrix, axis=1)))\n",
    "print(D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2560b",
   "metadata": {},
   "source": [
    "\n",
    "Aber wir brauchen $\\hat{D}$ nicht. Wir brauchen das Inverse dieser Matrix. Ohne $\\hat{D}^{-1}$ würde $\\hat{A}X$ die Features über alle verbundenen Knoten summieren. Dies würde dazu führen, dass Atome mit mehr Nachbarn größere Featurewerte haben. Durch die Einbeziehung von $\\hat{D}^{-1}$ werden die aggregierten Merkmale durch die Anzahl der benachbarten Atome gemittelt. `np.linalg.inv()` wird benötigt, um die Inverse der Matrix `D`.\n",
    "\n",
    "Bevor wir mit dem Aufbau eines Netzes beginnen, müssen wir einen weiteren Schritt durchführen. Um Rechenaufwand zu sparen, können wir die Berechnung $\\hat{D}^{-1}\\hat{A}$ bereits vor dem Training des Netzes berechnen. Dies kann Zeit sparen, da dieser Schritt nicht immer wieder im Netz wiederholt werden muss.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159329",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = []\n",
    "for i in range(len(D)):\n",
    "    DA.append(np.matmul(np.linalg.inv(D[i]),A[i]))\n",
    "DA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1fe5a8",
   "metadata": {},
   "source": [
    "Wir haben also die Liste der Adjacencymatrizen `DA`. Diese enthält die Informationen über die Struktur des Moleküls. Die Liste `feat` enthält die Features, d.h. die Informationen über die einzelnen Atome. Und schließlich erstellen wir eine Liste `labels`. Diese enthält die Informationen, die wir vorhersagen wollen (toxisch vs. nicht toxisch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DA = [torch.tensor(x,dtype=torch.float32) for x in DA] # Konvertieren der Arrays zu Tensoren\n",
    "feat = [torch.tensor(x,dtype=torch.float32) for x in feat] # Konvertieren der Arrays zu Tensoren\n",
    "labels = [torch.tensor([x], dtype=torch.float32) for x in data_tox['activity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd3802",
   "metadata": {},
   "source": [
    "## Graph Convolution Layer\n",
    "\n",
    "Wir wollen uns die Struktur von PyTorch für die Graph Convolution zunutze machen. Wie letzte Woche werden wir dafür Klassen verwenden.\n",
    "Letzte Woche haben wir eine Klasse benutzt, um ein Netzwerk aus verschiedenen Layern zu erstellen (z.B. `nn.Linear()`). \n",
    "Diese Layers sind in PyTorch bereits vorgegeben, aber wir können auch unsere eigenen Layers erstellen. Auch hierfür benötigen wir die PyTorch-Klasse.\n",
    "Unser Ziel ist es, so etwas wie `nn.Linear()` zu programmieren, damit wir unser Repertoire an Layers (Linear, RNN, GRU, Dropout, ...) um eine Graph Convolution erweitern können.\n",
    "\n",
    "Auch hierfür können wir die Klasse `nn.Module` als Basis für unsere Graph Convolution verwenden. Wir beginnen mit dem Minimalgerüst:\n",
    "\n",
    "\n",
    "```python\n",
    "class GraphConvolution(nn.Module):\n",
    "    pass\n",
    "```\n",
    "\n",
    "Damit wir richtig auf die Funktionen der übergeordneten `nn.Module` Klasse zugreifen können, müssen wir diese mit `super().__init__()` initialisieren. Unsere Convolutional Layer muss natürlich wissen, wie groß der Input ist, und wie groß der Output werden soll. Da wir jetzt nicht ein Netzwerk, sondern nur eine Layer definieren, müssen wir zusätzlich angeben wie die Weights und Biases zu initialisieren sind.\n",
    "\n",
    "```python\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_featuers = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "```\n",
    "\n",
    "Dann können wir auch schon die `forward()` Funktion programmieren. Hier passiert die eigentliche Convolution, bei der die Knoten mit den Features `x` durch Matrixmultiplikation mit der Adjacency Matrix `adj` über ihre Nachbarknoten aggregiert werden. Indem wir noch die lernbaren `weights` dazwischenschalten, können wir das Ganze später optimieren.\n",
    "\n",
    "```python\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "```\n",
    "Damit würde unsere `GraphConvolution` Klasse auch schon funktionieren. Wir initialisieren die Weights mit der Funktion `reset_parameters()`. Die Funktion `__repr__()` gibt graphisch wieder wie unsere Layer aussieht, nachdem sie initilaiert wurde. \n",
    "\n",
    "Unsere finale Klasse sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e85d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(0, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        return output + self.bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'in_features=' + str(self.in_features) + ', ' \\\n",
    "               + 'out_features=' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d289f",
   "metadata": {},
   "source": [
    "Wir können auch überprüfen, ob unsere Graph Convolution Layer auch schon funktioniert.\n",
    "Zunächst speichern wir eine Feature-Matrix und eine Adjacency Matrix als Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_beispiel = feat[1]\n",
    "adj_beispiel = DA[1]\n",
    "print('Features:', feat_beispiel.shape)\n",
    "print('DA:', adj_beispiel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b55b",
   "metadata": {},
   "source": [
    "Wir erstellen nun eine `GraphConvolution`. Beachten Sie, dass die Inputgröße, der ersten Layer der Größe des Featurevektors (Anzahl der Spalten) (`25`) entspricht. Also genauso wie bei einer `nn.Linear` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GraphConvolution(25, 100)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab305ee3",
   "metadata": {},
   "source": [
    "Wir können nun das Beispiel durch die Convolution führen. Sie werden sehen, dass sich die Dimension der Features auf 100 vergrößert haben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(feat_beispiel, adj_beispiel)\n",
    "print('\\nOutput:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a666",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "\n",
    "Um nun ein Netz zu erstellen, können wir wieder die Klasse `nn.Module` verwenden. \n",
    "\n",
    "Letzte Woche haben wir unseren eigenen Autoencoder zusammengestellt. Diese Woche verwenden wir diese Art von Klasse, um mehrere Graph Convolutions miteinander zu verbinden. Es ist jedoch wichtig zu beachten, dass wir, wie bei regulären CNN, nicht nur Convolution Layers verwenden können. Im CNN selbst müssen wir den Tensor am Ende \"flatten\". Dadurch erhalten wir einen Vektor, der dann durch eine lineare Layer geleitet wird. Das Gleiche gilt für Graph Convolutions. Um die Ausgabe dieser Graph Convolutions in eine lineare Layer zu leiten, können wir jedoch nicht einfach PyTorchs `flatten` verwenden, sondern wir werden den Durchschnitt Durchschnitt über alle Spalten berrechnen.\n",
    "\n",
    "Dies geschieht mit der Funktion `aggregate()`.\n",
    "\n",
    "Sie können im Netzwerk sehen, dass wir zwei Graph Convolution und dann eine lineare Layer verwenden.\n",
    "Im Forward Pass wird zunächst der Input `(x, adj)` durch die erste Graph Convolution geführt, dann folgt eine ReLU-Funktion. Das gleiche Spiel wird dann für die zweite Convolution wiederholt. \n",
    "Nun wenden wir die Funktion `aggregate` an. Diese errechnet nun den Mittelwert jedes Features. Der Output dieser Layer hat also im Beispiel die Größe `[1,100]`.\n",
    "Schließlich verwenden wir die lineare Layer, um die eigentliche Vorhersage zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):#in_features, out_features, size_labels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): # we use mean aggregation, max or min could also be used as hyperparameter\n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8283147",
   "metadata": {},
   "source": [
    "Nun werden wir den Datensatz schnell in einen Trainings- und einen Testsatz unterteilen. Dazu nehmen wir einfach die ersten 1800 Moleküle als Trainingset und den Rest als Testset. Es ist wichtig, dass wir in diesem Beispiel keine Minibtaches verwenden. Der Grund dafür ist, dass die Featurematrizen der einzelnen Moleküle unterschiedlich groß sind bzw. eine unterschiedliche Anzahl von Zeilen haben. Das bedeutet, dass wir sie nicht einfach als 3D-Tensor speichern können, wie es zum Beispiel bei CNNs der Fall ist. \n",
    "\n",
    "Es gibt Möglichkeiten, dieses Problem zu lösen, aber sie sind für dieses Notebook nicht relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = feat[:1800]\n",
    "train_DA = DA[:1800]\n",
    "train_labels = labels[:1800]\n",
    "\n",
    "\n",
    "test_feat = feat[1800:]\n",
    "test_DA = DA[1800:]\n",
    "test_labels = labels[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b44562",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GraphNN()\n",
    "loss_funktion= nn.BCEWithLogitsLoss()\n",
    "optimizer=optim.Adam(gnn.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75bda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    loss_list_train = []\n",
    "    acc_list_train= []\n",
    "    gnn.train()\n",
    "    for k in range(len(train_feat)):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output=gnn(train_feat[k], train_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,train_labels[k])\n",
    "        loss.backward()\n",
    "        loss_list_train.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_list_train.append(np.sum((torch.round(torch.sigmoid(output)) == train_labels[k]).detach().numpy()))\n",
    "    loss_list_test = []\n",
    "    acc_list_test= []\n",
    "    gnn.eval()\n",
    "    for k in range(len(test_feat)):\n",
    "    \n",
    "        output=gnn(test_feat[k], test_DA[k]).flatten()\n",
    "\n",
    "        loss=loss_funktion(output,test_labels[k])\n",
    "        loss_list_test.append(loss.item())\n",
    " \n",
    "\n",
    "        acc_list_test.append(np.sum((torch.round(torch.sigmoid(output)) == test_labels[k]).detach().numpy()))\n",
    "            \n",
    "        \n",
    "    print(i,\"Train Loss: %.2f Train Accuracy: %.2f Test Loss: %.2f Test Accuracy: %.2f\"\n",
    "        % (np.mean(loss_list_train), np.mean(acc_list_train),np.mean(loss_list_test), np.mean(acc_list_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d732ba",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, dauert das Training sehr lange und ist nicht sehr erfolgreich. Das hier vorgestellte Modell ist ein sehr, sehr einfaches Modell. In der Tat werden normalerweise komplexere Graphen Convolutions verwendet. Auch wird nicht nur der Atomtyp, sondern eine Vielzahl an Features als Input verwendet. \n",
    "\n",
    "Seit ein paar Jahren gibt es auch [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) und die [Deep Graph Library](https://www.dgl.ai/). Beide stellen eine Erweiterung zu PyTorch dar. Sie enthalten wichtige Funktionalitäten, die für den Umgang mit Graphen relevant sind. Außerdem enthalten die Libraries die wichtigsten Graph Layers. Auf diese Weise müssen Sie die Layers nicht selbst programmieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe796496",
   "metadata": {},
   "source": [
    "## Übungsaufgabe:\n",
    "\n",
    "Hier sehen Sie unser Graph Neural Network.\n",
    "Das Problem ist, dass dieses Netzwerk keine Flexibilität bietet.\n",
    "Die Weightmatrizen des Netzwerks werden immer die selber Größen haben.\n",
    "Auch fehlt Dropout. Wir brauchen kein Batchnorm, da wir keine Minibatches benutzen.\n",
    "\n",
    "\n",
    "```python\n",
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Können Sie das Netzwerk so umschreiben, dass wir flexibel den Input, als auch die größen der Graph Convolutions anpassen können?\n",
    "Sie können testen, ob ihr Netzwerk noch funktioniert mit dem `beispiel_DA` und `beispie_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "beispiel_DA = test_DA[0]\n",
    "beispiel_feat = test_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23046cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConvolution(25, 100)\n",
    "        self.conv2 = GraphConvolution(100, 100)\n",
    "        self.lin = nn.Linear(100, 1)\n",
    "        \n",
    "    def aggregate(self, convoluted_graph): \n",
    "        return torch.mean(convoluted_graph, dim=0, keepdim=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.aggregate(x)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
