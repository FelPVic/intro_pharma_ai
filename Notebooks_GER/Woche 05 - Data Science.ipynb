{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Toxikologische Vorhersagen\n",
    "\n",
    "\n",
    "---\n",
    "### Lernziele\n",
    "\n",
    "\n",
    "- Sie können einen SVM trainieren.\n",
    "- Sie verstehen den Grund für die Notwenigkeit Variablen zu skalieren.\n",
    "- Sie können ein Random Forest Modell trainieren.\n",
    "- Sie verstehen Y-Scrambling und die Notwendigkeit von Train/Test-Splits.\n",
    "- Sie können Daten in eine Trainings- und eine Testsatz aufteilen.\n",
    "---\n",
    "\n",
    "Heute werden Sie einige Grundlagen der Data Science und des maschinellen Lernens lernen. Diese werden später für das trainieren von neuronaler Netze relevant sein. \n",
    "Als Beispiel werden wir Modelle erstellen, die die toxikologische Bedenklichkeit von Molekülen erkennen sollen.\n",
    "Im konkreten Beispiel geht es um die Messung des **mitochondrialen Membranpotenzials** (MMP). Dieses wird als Indikator für die allgemeine Gesundheit der Zelle verwendet \\[1\\].\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/326685180/figure/fig4/AS:654070805172233@1532954040230/assay-of-a549-cells-mitochondrial-membrane-potential-with-Jc-1-staining-method-Notes.png\" width=400 height=200 />\n",
    "\n",
    "<center>Beispiel eines MMP Assays.<br> <i> Source: Liao et al.[2] licensed under CC-BY-NC</i></center>\n",
    "\n",
    "\n",
    "Die Daten stammen aus dem Datensatz der Tox21 Challenge. Bei diesem Wettbewerb ging es darum, die toxikologischen Eigenschaften von Molekülen vorherzusagen. Zu diesem Zweck wurden die Messungen für insgesamt zwölf Assays zur Verfügung gestellt. Wir konzentrieren uns vorerst nur auf einen Assay und verwenden auch nicht den gesamten Datensatz, sondern nur etwa 2000 Moleküle. \n",
    "\n",
    "\n",
    "##### Zuvor aber werden wir anhand eines Beispiels den Effekt von Variablen Scaling betrachten.\n",
    "\n",
    "---\n",
    "**Referenzen:**\n",
    "<br>\n",
    "<br>\n",
    "\\[1\\] Sakamuru, S., Attene-Ramos, M. S., & Xia, M. (2016). Mitochondrial membrane potential assay. In High-throughput screening assays in toxicology (pp. 17-22). Humana Press, New York, NY. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5375165/\n",
    "<br>\n",
    "<br>\n",
    "\\[2\\] Liao, C., Xu, D., Liu, X., Fang, Y., Yi, J., Li, X., & Guo, B. (2018). Iridium (III) complex-loaded liposomes as a drug delivery system for lung cancer through mitochondrial dysfunction. International Journal of Nanomedicine, 13, 4417."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "!pip install rdkit==2022.3.4\n",
    "!pip install seaborn\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from rdkit.Chem.Lipinski import * \n",
    "from rdkit.Chem.rdMolDescriptors import CalcExactMolWt, CalcTPSA\n",
    "from rdkit.Chem.Crippen import MolLogP, MolMR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as  sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline\n",
    "if 'google.colab' in sys.modules:\n",
    "  !wget https://raw.githubusercontent.com/kochgroup/intro_pharma_ai/main/utils/utils.py\n",
    "  %run utils.py\n",
    "else:\n",
    "  %run ../utils/utils.py # lädt vorgeschriebene Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "Die Skalierung von Variablen kann für den Erfolg von Machine Learning Modellen entscheidend sein. Skalierung von Variablen bedeutet, dass wir die Skala der Werte einer Variablen ändern. Genauer gesagt wollen wir, dass alle Variablen die gleiche Skala verwenden. \n",
    "\n",
    "Beispielsweise liegt der Wert eines Hauses in € oft zwischen 50.000 und mehreren Millionen. Die Fläche des Hauses beträgt jedoch wahrscheinlich nur zwischen 10 und mehreren hundert Quadratmetern. Die Werte, die der Preis annehmen kann, sind viel größer als die der Fläche. \n",
    "\n",
    "Für einige Algorithmen, die auf Entfernungen oder Gradienten beruhen, kann dies ein Problem darstellen, da die Skala der Variablen für diese Variable einen direkten Einfluss auf die Bedeutung der Variable hat.\n",
    "\n",
    "Variablen mit einer größeren Skala wird mehr Bedeutung beigemessen, auch wenn die Skala willkürlich ist. Man könnte auch den Preis eines Hauses in 1000€ ausdrücken (aus 50.000 wird 50). Die Skala ändert sich, aber nicht die eigentliche Variable.\n",
    "\n",
    "Um diesen Effekt zu vermeiden, skalieren wir Variablen auf einheitliche Größen.\n",
    "Zum Beispiel skaliert das **MinMax-Scaling** alle Werte zwischen '0' und '1'. \n",
    "\n",
    "Im folgenden Beispiel sehen Sie, welchen Einfluss die Skalierung auf eine Support Vector Machine haben kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst laden Sie die Daten mit Python ein. \n",
    "\n",
    "___\n",
    "**Dateitypen:**\n",
    "<br> <br>\n",
    "Es gibt viele verschiedene Formate, in denen Daten gespeichert werden. Das wohl am häufigsten verwendete ist das Format `comma separated value`. Dieses erkennt man an der Abkürzung `.csv` am Ende einer Datei.  <div style=\"float: right;\"><img  src=\"https://images.freeimages.com/images/large-previews/7f6/tab-key-1243535.jpg\" width=150 height=100 /><center>freeimages.com: T. Al Nakib</center></div>\n",
    "\n",
    "Wie der Name schon vermuten lässt, werden die einzelnen Werte durch ein Komma getrennt. Werte, die in dieselbe Zeile gehören, werden in dieselbe Zeile geschrieben und durch das Komma getrennt. Andere Dateiformate, die häufig verwendet werden, sind Textdateien `.txt`. Hier kann man nicht direkt aus der Endung schließen, wie die Struktur aufgebaut ist.\n",
    "Oft wird aber das System \"Werte in einer Zeile gehören in eine Zeile\" eingehalten. Lediglich das Zeichen, das einzelne Werte trennt, kann sich unterscheiden. Ein häufig verwendetes Trennzeichen ist der Tab(ulator). Das Tabulatorzeichen wird auch oft im `.smi` Format verwendet. Sie werden dieses Format häufiger sehen, wenn Sie mit SMILES arbeiten. Wenn Sie sich nicht sicher sind, welcher \"seperator\" verwendet wird, können Sie die Datei mit einem einfachen Texteditor öffnen. Unter Windows zum Beispiel \"Notepad\". Hier sollten Sie sehen, wie die Werte voneinander getrennt sind.\n",
    "\n",
    "In diesem Fall werden die Daten im Format `.tab` gespeichert. Die Werte werden also durch einen Tab getrennt. Sie können die Funktion `pd.read_csv()` auch dann verwenden, wenn die Datei keine `.csv`-Datei ist. \n",
    "Aber dann müssen Sie zusätzlich den `Seperator` `sep= \"\\t\"` angeben. Das Symbol `\"\\t\"` wird dann als Tab erkannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ab\")\n",
    "print(\"a\\tb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie vielleicht schon von einigen der geladenen Libraries gesehen haben, werden wir heute einige Funktionen verwenden, die Ihnen noch unbekannt sind. \n",
    "\n",
    "- `pandas` enthält nützliche Funktionen für die Verarbeitung großer Datenmengen und kann so ziemlich alles, was Excel auch kann (einiges sogar besser). `pandas` erstellt sogenannte `DataFrames`. DataFrames sind ähnlich zu 2D `arrays` in `numpy` und speichern Daten in Zeilen und Spalten. Der Unterschied ist, dass wir verschiedene Arten von Daten in einem `DataFrame` speichern können. Zum Beispiel `floats` und `strings` in zwei verschiedenen Spalten. Wir können den Spalten und Zeilen auch Namen zuweisen, um einen besseren Überblick über unsere Daten zu haben.\n",
    "\n",
    "- Sie kennen `sklearn` bereits aus dem ROC-AUC (Woche 04).  `sklearn` bringt viele Funktionen mit, die für die Aufbereitung von Daten wichtig sind. Außerdem gibt es in `sklearn` mehrere maschinelle Lernalgorithmen, darunter den Random Forest und Support Vector Machines.\n",
    "\n",
    "In der folgenden Zelle werden die Daten für ein einfaches Beispiel eingelesen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_beispiel = pd.read_csv(\"https://uni-muenster.sciebo.de/s/XTC29EBCPfMfqqh/download\", sep = \"\\t\")\n",
    "\n",
    "print(\"Type:\", type(toy_beispiel))\n",
    "print(\"Shape:\",toy_beispiel.shape)\n",
    "toy_beispiel.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die mit `pandas` eingelesenen Daten werden zunächst in einem `DataFrame` gespeichert. Dies ist eine Tabelle mit Zeilen und Spalten. Mit `.head()` können Sie die ersten 5 Zeilen eines `DataFrame` anzeigen. Die Daten enthalten drei Spalten: `x1` `x2` und `y`. Insgesamt enthält die Datei 150 Einträge. Also insgesamt 150 Messpunkte. \n",
    "\n",
    "`y` gibt die fiktive Zugehörigkeit der einzelnen Datenpunkte zu einer von zwei Klassen an.\n",
    "\n",
    "Um eine bessere Übersicht zu bekommen, erstellen wir ein Diagramm für die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(toy_beispiel.x1, toy_beispiel.x2,\"o\")\n",
    "plt.plot( toy_beispiel.x1[toy_beispiel.y==1],  toy_beispiel.x2[toy_beispiel.y==1],\"o\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, wird eine Klasse von der anderen \"eingeschlossen\". Wir wollen eine SVM trainieren, um die beiden Klassen zu unterscheiden.\n",
    "\n",
    "Sie sollten auch feststellen, dass sich die Skalen der beiden Eingangsvariablen `x1` und `x2` deutlich unterscheiden. \n",
    " \n",
    "\n",
    "`x1` Werte liegen ungefähr zwischen `-1` und `1`. <br>`x2` Werte befinden sich zwischen ca. `8` und `12`.\n",
    "\n",
    "Wir werden die Daten zunächst nicht skalieren und direkt eine SVM auf diese Daten trainieren. Hierfür bietet das Modul `sklearn.svm` einige Funktionalitäten. Wie bei der linearen Regression wird mit `sklearn` zunächst das Modell erstellt und dann trainiert.\n",
    "Zur Klassifizierung kann die Funktion `SVC` (Support Vector Classification) verwendet werden. Zuerst erstellen wir eine Variable `model`. Diese enthält den Support Vector Classifier (`SVC`). Um ihn zu trainieren, verwenden wir die Funktion `.fit(x,y)`, um den Klassifikator an unsere Daten anzupassen. \n",
    "\n",
    "Bis jetzt haben wir nur den SVC trainiert, um seine Vorhersagen zu erhalten, müssen wir wieder die Funktion `model.predict(x)` verwenden.\n",
    "\n",
    "\n",
    "**Wichtig:** Sie haben es vielleicht oben schon gesehen, in `pandas` können wir Variablen (d.h. Spalten) direkt aus dem `DataFrame` auswählen. So selektiert `toy_beispiel.y` die Spalte `y` aus dem DataFrame `toy_beispiel`. \n",
    "Wenn Sie Werte mit der klassischen Indizierung auswählen wollen, wie bei `arrays`, müssen Sie zuerst ein `.iloc[]` an den Namen des DataFrame anhängen. Die Indizierung funktioniert dann genauso wie bei `numpy`.\n",
    "\n",
    "`toy_beispiel.iloc[:,:2]` wählt die ersten beiden Spalten aus, also `x1` und `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(toy_beispiel.iloc[:,:2], toy_beispiel.y)\n",
    "y_pred = model.predict(toy_beispiel.iloc[:,:2])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Güte der Vorhersagen zu beurteilen, können Sie wieder die Accuracy/Genauigkeit verwenden. Dazu können Sie die gleiche Funktion wie in der letzten Woche verwenden. Berechnen Sie die Accuracy für die Vorhersagen dieses Modells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)\n",
    "\n",
    "accuracy(_____, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "accuracy(toy_beispiel.y, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Accuracy liegt bei etwa `0.7`. Nicht schlecht, aber es gibt noch Raum für Verbesserungen. Mit einer der vorgeschriebenen Funktionen in `utils.py` können Sie auch die Entscheidungsgrenzen sichtbar machen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(toy_beispiel, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können sehen, dass die Entscheidungsgrenze sehr lang ist. Das liegt daran, dass die Skala von `x2` größer ist. Das bedeutet, dass der Abstand von der Entscheidungsgrenze zu den Datenpunkten zweier Klassen (der maximiert werden muss) für `x2` leichter zu maximieren ist als für `1`. Deshalb sehen wir eine gute Entscheidungsgrenze für die Werte von `x2`, aber nicht für `x1`. \n",
    "\n",
    "Um dies zu ändern, können wir versuchen, die Daten zu skalieren.\n",
    "Dazu verwenden wir den so genannten `MinMax`-Skalierer, bei dem alle Werte zwischen `0` und `1` skaliert werden. Wir wenden diesen Skalierer auf beide Eingabevariablen (`x1`, `x2`) an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "toy_beispiel.x1 = min_max(toy_beispiel.x1)\n",
    "toy_beispiel.x2 = min_max(toy_beispiel.x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(toy_beispiel.x1, toy_beispiel.x2,\"o\")\n",
    "plt.plot( toy_beispiel.x1[toy_beispiel.y==1],  toy_beispiel.x2[toy_beispiel.y==1],\"o\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Plot sieht genau so aus wie der früher, aber die Skalen, also die x- und y-Achse, haben sich geändert. \n",
    "Das heißt, die relative Beziehung der Werte hat sich nicht geändert.\n",
    "Können Sie nun ein `model_2` mit `SVC` erstellen, das mit den skalierten Werten trainiert wird? Berechnen Sie auch die Accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = ____\n",
    "model_2.fit(_____, ______)\n",
    "y_pred = model_2.predict(toy_beispiel._____)\n",
    "accuracy(______,_____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "model_2 = SVC()\n",
    "model_2.fit(toy_beispiel.iloc[:,:2], toy_beispiel.y)\n",
    "y_pred = model_2.predict(toy_beispiel.iloc[:,:2])\n",
    "accuracy(toy_beispiel.y, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allein durch die Skalierung der Inputvariablen ergibt sich eine Verbesserung der Genauigkeit um 0,3.\n",
    "Eine deutliche Verbesserung ist auch anhand der Entscheidungsgrenze zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_svc(toy_beispiel, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch wenn die Daten nur für dieses Beispiel generiert wurden, zeigt es doch, warum die Skalierung der Inputvariablen so wichtig ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Split\n",
    "\n",
    "\n",
    "Von einem theoretischen Beispiel kommen wir nun zu einer praktischen Aufgabe: einer toxikologischen Vorhersage.\n",
    "Die ursprünglich erwähnten Assay Daten können mit `pd.read_csv()` direkt aus dem Internet geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/filipsPL/tox21_dataset/master/compounds/sr-mmp.tab\", sep = \"\\t\")\n",
    "\n",
    "print(\"Shape:\",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten enthalten drei Spalten: `Compound`, `SMILES` und `activity`. Insgesamt enthält die Datei 2246 Moleküle.\n",
    "\n",
    "- `Compound` enthält die ID, mit der die Werte im ursprünglichen Datensatz unterschieden werden können.\n",
    "- `SMILES` enthält die SMILES-`strings`.\n",
    "- `activity` enthält die Ergebnisse des Assays. Ein Molekül ist aktiv (`1`) oder inaktiv (`0`).\n",
    "\n",
    "---\n",
    "Wenn Sie wissen wollen, wie viele aktive und damit giftige Moleküle in dem Datensatz enthalten sind, können Sie einfach die Summe der Spalte `activity` berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data.activity) # data.activity wählt die in 'data' enthalten Spalte 'activity' aus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Analyse ist die `Compounds` Spalte nicht wichtig. Darum wird sie aus `data` entfernt. Danach benennen wir noch die Spalten neu, damit alle Namen kleingeschrieben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:,1:] #alle Spalten bis auf die erste (index 0) werden ausgewählt\n",
    "data.columns = [\"smiles\", \"activity\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben Daten, aber noch keinen Input für ihr Modell. SMILES sind `str`-Variablen, aber Modelle benötigen numerische Variablen als Eingabe. Das bedeutet, dass Sie noch so genannte Features erstellen müssen. Hierfür können Sie die Deskriptoren aus dem \"Cheminformatics\"-Notebook verwenden. In den folgenden Zellen wandeln wir die SMILES in `mol` um und verwenden sie zur Berechnung der Deskriptoren. \n",
    "\n",
    "Diesmal berechnen wir auch die molare Refraktivität (Maß für die Gesamtpolarisierbarkeit), die Anzahl der drehbaren Bindungen und die TPSA (topologische polare Oberfläche). Dann kombinieren wir alle Variablen in einem `DataFrame`.\n",
    "\n",
    "Passen Sie die `for-loops` an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wir konvertieren all SMILES zu mols\n",
    "mols = np.array([Chem.MolFromSmiles(x) for x in ________ ]) # Welche SMILES müssen ausgewählt werden\n",
    "\n",
    "# 1) N hydrogen bond donors\n",
    "num_hb_donors = [NumHDonors(x) for x in _______ ] # durch welche Variable loopen wir\n",
    "\n",
    "# 2) Hydrogen bond acceptors\n",
    "num_hb_acceptors = [NumHAcceptors(x) for _____ in ______] # durch welche Variable loopen wir\n",
    "\n",
    "# 3) Number of rotable bonds \n",
    "num_rotablebonds = [NumRotatableBonds(x) for x in mols]\n",
    "\n",
    "# 4) Molecular Mass: CalcExactMolWt()\n",
    "mw = [ _________(___ ) for x in mols]  # Berechnen Sie die das Gewicht mit CalcExactMolWt()\n",
    "\n",
    "# 5) log P: MolLogP()\n",
    "logP = [________ ___ __ __ ______] # Berechnen Sie den logP mit MolLogP()\n",
    "\n",
    "# 6) Molar refractivity \n",
    "mr = [MolMR(x) for x in mols]\n",
    "\n",
    "# 7) Polar Surface\n",
    "tpsa = [CalcTPSA(x) for x in mols]\n",
    "\n",
    "aux_data=pd.DataFrame({\n",
    "    \"hb_donors\": num_hb_donors,\n",
    "    \"hb_acceptors\": num_hb_acceptors,\n",
    "    \"rotable_bonds\": num_rotablebonds,\n",
    "    \"mw\": mw,\n",
    "    \"logP\": logP,\n",
    "    \"mr\":mr,\n",
    "    \"tpsa\":tpsa\n",
    "    })\n",
    "\n",
    "aux_data[\"activity\"] = data.activity # Wir fügen noch die activity hinzu\n",
    "aux_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "# Wir konvertieren all Smiles zu mols\n",
    "mols = np.array([Chem.MolFromSmiles(x) for x in data.smiles])\n",
    "\n",
    "# 1) N hydrogen bond donors\n",
    "num_hb_donors = [NumHDonors(x) for x in mols]\n",
    "\n",
    "# 2) Hydrogen bond acceptors\n",
    "num_hb_acceptors = [NumHAcceptors(x) for x in mols]\n",
    "\n",
    "# 3) Number of rotable bonds \n",
    "num_rotablebonds = [NumRotatableBonds(x) for x in mols]\n",
    "\n",
    "# 4) Molecular Mass\n",
    "mw = [CalcExactMolWt(x) for x in mols]\n",
    "\n",
    "# 5) log P\n",
    "logP = [MolLogP(x) for x in mols]\n",
    "\n",
    "# 6) Molar refractivity \n",
    "mr = [MolMR(x) for x in mols]\n",
    "\n",
    "# 7) Polar Surface\n",
    "tpsa = [CalcTPSA(x) for x in mols]\n",
    "\n",
    "aux_data=pd.DataFrame({\n",
    "    \"hb_donors\": num_hb_donors,\n",
    "    \"hb_acceptors\": num_hb_acceptors,\n",
    "    \"rotable_bonds\": num_rotablebonds,\n",
    "    \"mw\": mw,\n",
    "    \"logP\": logP,\n",
    "    \"mr\":mr,\n",
    "    \"tpsa\":tpsa\n",
    "    })\n",
    "\n",
    "aux_data[\"activity\"] = data.activity # Wir fügen noch die activity hinzu\n",
    "aux_data\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `DataFrame` `aux_data` hat für jeden Deskriptor eine eigene Spalte, also insgesamt sieben. Die erste Zeile enthält die Deskriptoren für das erste Molekül, die zweite für das zweite, und so weiter.\n",
    "Sie können nun diesen Datensatz verwenden, um ein Modell zu erstellen. \n",
    "Es sind jedoch einige zusätzliche Schritte erforderlich, bevor Sie ein funktionierendes Modell erstellen und Vorhersagen treffen können.\n",
    "\n",
    "Zunächst haben wir zwei Arten von Variablen. Variablen wie die Anzahl der drehbaren Bindungen werden als \"diskret\" bezeichnet, weil sie nur ganze Zahlen enthalten. Es gibt keine `3,5` drehbaren Bindungen in einem Molekül. Im Gegensatz dazu sind Variablen wie logP oder TPSA \"kontinuierlich\", d.h. Variablen, die Werte über den gesamten Zahlenbereich annehmen können.\n",
    "\n",
    "Darüber hinaus haben wir Variablen mit unterschiedlichen Skalen. Die Werte für das Gewicht sind viel größer als beispielsweise die Werte für logP oder die Anzahl der HB-Akzeptoren. \n",
    "\n",
    "Da wir jedoch als Nächstes ein Random-Forest-Modell verwenden wollen, müssen die Variablen nicht skaliert werden.\n",
    "Das liegt daran, dass Random Forests keine Abstände oder Gradienten verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst teilen wir den Datensatz in 'x' und 'y' also input und output\n",
    "x = aux_data.iloc[:,:7].values #'[:,:7]' Alle Spalten bis auf die 7 Spalte\n",
    "y = aux_data.iloc[:,7].values #'[:,7]' Nur Spalte 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die neue Variable `x` ist jetzt ein `np.array` (anstelle eines `DataFrame`). Das wurde durch die Endung `.values` möglich gemacht. So kann mann schnell `pd.DataFrame` zu `np.arrays` konvertieren. \n",
    "\n",
    "Sie können nun ein Random Forest Modell trainieren. Ähnlich wie bei `SVC` müssen Sie zuerst eine Variable mit dem Modell erstellen und dann `.fit()` verwenden, um das Modell auf den Daten zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42) \n",
    "# n_estimators wählt aus wie viele Trees benutzt werden sollen\n",
    "rf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu sehen, wie gut das Modell funktioniert, müssen Sie noch die Vorhersagen des Modells für unsere Daten extrahieren. Anders als üblich, verwenden wir dafür die Funktion <br>`.predict_proba(x)[:,1]`. Mit dem trainierten `rf` Modell machen wir die Vorhersagen für `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=rf.predict_proba(x)[:,1]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die vorhergesagte Wahrscheinlichkeit für das erste Molekül ist `0.014`. Wie bei der logistischen Regression bedeutet dies, dass das Molekül nach unserem Modell in 1,4 % der Fälle bei dem MMP-Test aktiv ist, d. h. es ist unwahrscheinlich, dass es toxisch ist. Je höher die Wahrscheinlichkeit ist, desto wahrscheinlicher ist es (laut dem Modell), dass das Molekül im Assay aktiv ist. \n",
    "Häufig wird 0,5 als \"Cut-off\"-Wert gewählt. Bei einem Wert von 0,5 würden wir also davon ausgehen, dass das Modell diese Moleküle als toxisch einstuft. \n",
    "\n",
    "Um besser beurteilen zu können, wie gut das Modell funktioniert, vergleichen Sie die vorhergesagten Werte `pred_y` mit den tatsächlichen Werten `y`.\n",
    "\n",
    "Dazu kann man wieder die Accuracy nehmen. Um die Accuracy zu berechnen, muss man zunächst die Wahrscheinlichkeiten auf `0` und `1` runden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann können wir den AUC mit der Funktion `roc_auc_score()` berechnen.\n",
    "Anders als bei `accuracy` werden hier die Wahrscheinlichkeiten `y_hat` anstelle der gerundeten Werte `y_pred` verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y, ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "roc_auc_score(y,y_hat)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut. Das Modell ist nahezu perfekt bei der Vorhersage. Es trifft in 99 % der Fälle die richtige Entscheidung. In der folgenden Zelle werden die falsch klassifizierten Moleküle ausgewählt und mit ihrer vorhergesagten Wahrscheinlichkeit angezeigt. Es ist nicht notwendig, dass Sie diesen Code verstehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falsch_klassifizierte=np.where(y_pred!=y)[0]\n",
    "Draw.MolsToGridImage(mols[falsch_klassifizierte],\n",
    "                     legends=[\"Vorhergesagte Wahrscheinlichkeit:\\n\"+str(np.round(x,3)) for x in y_hat[falsch_klassifizierte]],\n",
    "                     useSVG=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auffallend ist, dass die Wahrscheinlichkeiten für diese Moleküle meist relativ nahe bei 0,5 liegen. Das bedeutet, dass das Modell für diese Moleküle nicht sehr sicher war. Insgesamt wurden jedoch nur 19 Moleküle falsch klassifiziert, so dass wir uns nicht allzu viele Sorgen machen sollten.\n",
    "\n",
    "\n",
    "## Y-Scrambling\n",
    "\n",
    "##### Das Problem:\n",
    "\n",
    "*Hat das Modell wirklich gelernt, was für den MMP-Assay wichtig ist. Oder hat sich das RF-Modell nur unsere Daten auswendig gelernt?*.\n",
    "\n",
    "Wir können dies mit einem einfachen Test herausfinden. Wir trainieren das RF-Modell erneut, aber mischen wir die Variable `activity` zufällig. Das heißt, die echten Messungen werden gemischt und neu, zufällig auf die Moleküle verteilt. Dieser Vorgang wird auch als **Y-scrambling** bezeichnet. \n",
    "\n",
    "Nehmen wir an, unsere realen Daten sehen wie folgt aus:\n",
    "\n",
    "smiles|Deskriptor 1| Deskriptor 2|activity\n",
    "------|------------|-------------|--------\n",
    "SMILES 1|$x_{1,1}$ |$x_{1,2}$|$y_1$\n",
    "SMILES 2|$x_{2,1}$ |$x_{2,2}$|$y_2$\n",
    "SMILES 3|$x_{3,1}$ |$x_{3,2}$|$y_3$\n",
    "SMILES 4|$x_{4,1}$ |$x_{4,2}$|$y_4$\n",
    "\n",
    "Für jedes SMILES wurden zwei Deskriptoren berechnet. Wir haben auch die Aktivität ($y_1$-$y_4$) für jedes Molekül aufgezeichnet. Die Aktivität $y_1$ ist die gemessene Aktivität von SMILES 1 und so weiter.\n",
    "\n",
    "Nach dem *Y-Scrambling* sehen unsere Daten wie folgt aus:\n",
    "\n",
    "smiles|Deskriptor 1| Deskriptor 2|activity\n",
    "------|------------|-------------|--------\n",
    "SMILES 1|$x_{1,1}$ |$x_{1,2}$|$y_2$\n",
    "SMILES 2|$x_{2,1}$ |$x_{2,2}$|$y_3$\n",
    "SMILES 3|$x_{3,1}$ |$x_{3,2}$|$y_4$\n",
    "SMILES 4|$x_{4,1}$ |$x_{4,2}$|$y_1$\n",
    "\n",
    "Die $y$ Werte wurden zufällig anderen Molekülen zugeordnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Y-Scrambling führt zum Verlust der tatsächlichen Beziehungen, die zwischen den Variablen `x` (d.h. unseren Deskriptoren wie logP,...) und der zu vorhersagenden Variable `y` bestehen.  Das liegt daran, dass die Beziehung jetzt nur noch zufällig ist. Wenn unser Random Forest tatsächlich Muster lernt, anstatt die Daten auswendig zu lernen, dann sollte das Modell bei diesem Datensatz schlechter abschneiden.\n",
    "\n",
    "Um dies auszuprobieren, benötigen Sie die Bibliothek `random`. Die Funktion `random.shuffle()` sortiert die Werte von `y` zufällig neu. Sie können dann das Random-Forest Modell erneut trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(15) #ein Seed stell sicher das Sie alle die selbe zufälligen Daten erhalten\n",
    "y_random=np.array(y) # erst speichern wir y in einem array\n",
    "random.shuffle(y_random) # dann shuffeln wir y_random, wir müssen diese Variable nicht extra speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben gerade die Aktivitätsinformationen neu gemischt. Jetzt können wir ein Random-Forest Modell trainieren. Dieses Mal verwenden wir jedoch nicht `y`, sondern `y_random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Modell wird neu trainiert\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(x, ______) # Welcher y Variable brauchen Sie?\n",
    "y_hat=rf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(x, y_random) # Welcher y Variable brauchen Sie\n",
    "y_hat=rf.predict_proba(x)[:,1]\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt berechnen Sie erneut die Accuracy (achten Sie darauf, dass wir wieder `y_random` und nicht `y` für `y_true` benutzen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(y_random, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Tat verschlechtert sich die Accuracy nach dem Y-Scrambling. Aber die Accuracy liegt immer noch bei über 90 %. Das Random-Forest-Modell ist immer noch relativ gut bei der Vorhersage von toxikologischer Bedenklichkeit, auch wenn die von Ihnen verwendeten Daten überhaupt keinen Sinn mehr machen. Das Modell hätte überhaupt nicht mehr lernen können, da kein Zusammenhang zwischen `x` und `y` existiert. Das RF-Modell hat seine Accuracy also nur durch Auswendiglernen erreicht. \n",
    "\n",
    "Aus diesem Grund wird immer ein Testdatensatz verwendent. Dieser Testdatensatz wird beim Training nicht verwendet, und so sieht das Modell diese Moleküle zum ersten Mal, wenn wir die Qualität des Modell evaluieren wollen. Das Auswendiglernen der Moleküle im Trainingsdatensatz kann immer noch passieren, hilft dem Modell aber nicht bei dem Molekülen die wir im Testdatensatz haben.  \n",
    "\n",
    "---\n",
    "Häufig werden Datensätze nicht nur in Trainings-/Testsets, sondern in Trainings-/Validierungs-/Testsets aufgeteilt.\n",
    "Die Modelle werden dann auf der Grundlage des Validierungssets optimiert und nur das optimierte Modell wird dann auf dem Testset getestet.\n",
    "In der Chemieinformatik wird das Validierungsset manchmal auch als Testset bezeichnet. Das eigentliche Testset wird dann als externers Validierungsset bezeichnet.\n",
    "\n",
    "---\n",
    "Auch hier gibt es Funktionen, die Ihnen die Arbeit abnehmen. Die Funktion `train_test_split` teilt die Daten in ein Testset und eine Trainingsset auf. Wir verwenden 80 % des Datensatzes für das Training und den Rest für die Validierung. Die Moleküle werden dann nach dem Zufallsprinzip auf die beiden Sets aufgeteilt. Dann werden die Daten erneut in `x` und `y` aufgeteilt, aber diesmal für `train` und `test` getrennt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "\n",
    "train_x = train.iloc[:,:7]\n",
    "train_y = train.iloc[:,7]\n",
    "test_x = test.iloc[:,:7]\n",
    "test_y =  test.iloc[:,7]\n",
    "f\"Train Shape: {train.shape}, Test Shape: {test.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Trainingsset enthält nur noch 1796 Moleküle und das Testset 450. \n",
    "\n",
    "Wir trainieren zunächst den Random Forest nur mit den Trainingsdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Vorhersagen machen wir aber nur für die Moleküle im Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=rf.predict_proba(test_x)[:,1]\n",
    "y_pred = np.round(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können jetzt auch die Accuracy berechnen. Welche Variable brauchen wir nun für `y_true`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(___, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Lösung:</b></summary>\n",
    "\n",
    "```python\n",
    "accuracy(test_y, y_pred)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Genauigkeit hat sich um einiges verschlechtert, ist aber immer noch gut. Dieses Mal können wir jedoch sicher sein, dass die Leistung nicht auf das Auswendiglernen zurückzuführen ist, da das Modell diese Moleküle noch nie gesehen hat. Wenn wir nun den Y-Scrambling anwenden, sollte sich die Leistung drastisch verschlechtern. \n",
    "Wir ersetzen die `aux_data.activity` durch die zuvor erstellten `y_random`. Dies sind die gemischten `y`-Werte und wir wiederholen die Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data.activity = y_random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann teilen wir die Daten erneunt in Trainings- und Testset ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random, test_random=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "train_x_random = train_random.iloc[:,:7]\n",
    "train_y_random = train_random.iloc[:,7]\n",
    "test_x_random = test_random.iloc[:,:7]\n",
    "test_y_random =  test_random.iloc[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wiederholen das Training mit den randomisierten Daten. Dann lassen wir das Modell Vorhersagen für das Testsset machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x_random, train_y_random)\n",
    "y_hat=rf.predict_proba(test_x_random)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Schluss berechnen wir die Genauigkeit (Accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(y_hat)\n",
    "accuracy(test_y_random, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Benutzen eines Testset fällt die Genauigkeit des Modells mit den Y-scrambled Daten drastisch auf ungefähr 50 %. Es ist nicht besser als ein Model, das einfach raten würde.\n",
    "Nur durch die Verwendung eines Testsets konnten wir zeigen, dass das Modell etwas gelernt hat, das über das Auswendiglernen hinausgeht\n",
    "Es ging vor allem darum, zu zeigen, wie wichtig es ist, ein Modell nicht anhand des Trainingssets zu bewerten. \n",
    "\n",
    "Y-Scrambling wird in der Praxis selten verwendet. Aber die OECD beispielsweise verlangt einen Y-Scrambling-Test zur Validierung von QSAR-Modellen (Quantitative Structure-Activity Relationship)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Als letzten Schritt wollen wir uns noch die **Feature Importance** anschauen. Die Feature Importance gibt an, wie wichtig die einzelnen Inputvariablen für die Entscheidung sind. Je nachdem, welchen ML Algorithmus Sie verwenden, können Sie die Feature Importance relativ leicht extrahieren. Wir trainieren unseren RF zunächst erneut, diesmal mit einem Datensplit.\n",
    "\n",
    "In der nächsten Zelle wird zunächst das Random-Forest Modell erneut trainiert (ohne y-scrambling). Dannach können wir usn die Feature Importance von dem Modell ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data.activity = data.activity\n",
    "\n",
    "train, test=train_test_split(aux_data,test_size= 0.2, train_size= 0.8, random_state=1234)\n",
    "train_x = train.iloc[:,:7]\n",
    "train_y = train.iloc[:,7]\n",
    "test_x = test.iloc[:,:7]\n",
    "test_y =  test.iloc[:,7]\n",
    "\n",
    "\n",
    "# Train Model\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_x, train_y)\n",
    "y_hat=rf.predict_proba(test_x)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index=aux_data.columns.values[:-1])\n",
    "feat_importances.nlargest(20).nsmallest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird deutlich ersichtlich, dass der LogP der wichtigste Parameter für die Bestimmung der Toxizität ist, während die Anzahl an H-Brücken-Donoren und -Akzeptoren weniger relevant sind. \n",
    "\n",
    "Das der LogP Wert wichtig ist, ist nicht überraschend. Wir zum Beispiel die Density Plots von aktiven und inaktive Molekülen anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(aux_data.logP[aux_data.activity==1], color=\"red\")\n",
    "sns.kdeplot(aux_data.logP[aux_data.activity==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sehen wir einen deutlichen Trend. Bei höherem LogP ist das Molekül eher aktiv. \n",
    "\n",
    "Probieren Sie es selbst mit den anderen Deskriptoren aus(`hb_donors`, `hb_acceptors`, `rotable_bonds`, `mw`, `mr`, `tpsa`). Welche Deskriptoren haben neben dem LogP unterschiedliche Verteilung basierend auf der Aktivität?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgabe \n",
    "\n",
    "Sie haben bereits Fingerabdrücke als Molekülrepräsentation kennengelernt. Da sie leicht zu berechnen sind und immer eine feste Länge haben, eignen sie sich gut als Input für ML Modelle. Allerdings sind Fingerabdrücke für den Menschen nicht so einfach zu interpretieren.\n",
    "\n",
    "Ihre Aufgabe wird es sein, erneut ein Random Forest Modell zu trainieren. Diesmal werden Sie den ECFP4 als Input verwenden.\n",
    "\n",
    "Für Sie wurde  die Funktion `get_fingerprints()` bereits vorgeschrieben. Mit ihr können Sie Fingerabdrücke aus den SMILES berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fps = get_fingerprints(data)\n",
    "fps[\"activity\"] = data.activity\n",
    "fps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fps` enthält insgesamt 2049 Spalten. 2048 davon sind die jeweiligen Bits des Fingerabdrucks. Die letzte Spalte enthält die `activity`.\n",
    "\n",
    "Zunächst wird der Datensatz in `training` und `test` unterteilt. 80 % der Daten sollten im Trainingsset und 20 % im Testset enthalten sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(_____,test_size= ___ , train_size= ______, random_state=1234)\n",
    "\n",
    "train_x = __________\n",
    "train_y = __________\n",
    "test_x = __________\n",
    "test_y = __________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Aufteilung der Daten trainieren Sie einen Random Forest Classifier mit dem Trainingsdatensatz.\n",
    "Anschließend verwenden Sie das trainierte Modell, um die Moleküle im Testdatensatz `test` zu klassifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(_____, _____)\n",
    "y_hat=rf.predict_proba(______)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(___,____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns auch noch einmal die Feature Importance ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index=range(2048))\n",
    "feat_importances.nlargest(20).nsmallest(20).plot(kind='barh', title = \"Importance of Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider lässt sich dieser Plot nicht mehr so gut interpretieren, auch wenn deutlich wird, dass die obersten fünf Bits für die Aktivitätsvorhersage wichtig sind.\n",
    "\n",
    "Die Bits können auch nicht so gut dargestellt werden. Mit RDKit können wir aber die Substrukturen darstellen, die jeden Bit zugeordnet sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_important_bits = feat_importances.nlargest(20).index.values\n",
    "print(\"Die 20 wichtigsten bits:\", most_important_bits)\n",
    "mol_ll = []\n",
    "bi_ll = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    bit = most_important_bits[i]\n",
    "    for x in data.smiles:\n",
    "        bi ={}\n",
    "        mol = Chem.MolFromSmiles(x)\n",
    "        fp = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, bitInfo=bi)\n",
    "        if np.sum(np.array(list(bi))==bit)>0:\n",
    "            mol_ll.append(mol)\n",
    "            bi_ll.append(bi)\n",
    "            break\n",
    "        \n",
    "prints=[(mol_ll[i],most_important_bits[i], bi_ll[i]) for i in range(20)]\n",
    "\n",
    "Draw.DrawMorganBits(prints, useSVG=True, molsPerRow=3, legends= [str(most_important_bits[i]) for i in range(20)], subImgSize= [300,300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das wichtigste Bit ist für unsere Daten eine phenolische Hydroxygruppe (aromatische Atome sind gelb hervorgehoben, das zentrale Atom ist blau). Auch sonst sind viele aromatische Fragmente in den wichtigsten Bits vertreten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
